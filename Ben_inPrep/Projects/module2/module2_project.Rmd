---
title: "module2_project"
output: html_document
date: "2023-11-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(tidyverse)
library(stringr)
library(tidytext)
library(ggplot2)
library(textdata)
library(textrecipes)
```

# Load the data
## Wikipedia talk page comments, labeled for toxicity by Amazon Turk workers
```{r}
rm(list = ls(all = TRUE))

# Since each comment was rated by several workers, we'll take the mean of the "toxicity_score" variable for each "rev_id". Toxicity ranges from -2 (most toxic) to +2 (most positive)
annotations = read_tsv('/Users/jfalanda/Documents/Courses/BDSII/Content/Projects/Module2/toxicity_annotations.tsv') %>% group_by(rev_id) %>% summarise(toxicity_score = mean(toxicity_score))

# Load the comments (text strings) themselves
comments = read_tsv('/Users/jfalanda/Documents/Courses/BDSII/Content/Projects/Module2/toxicity_annotated_comments.tsv')

# Join the toxicity scores with the text data.
## Also, we create a new factor called "toxic", which is 1 if toxicity_score < 0, or 0 otherwise
df = left_join(comments, annotations, by = c("rev_id")) %>% mutate(toxic = factor(ifelse(toxicity_score < 0, 1, 0)))

# remove the un-merged dataframes
rm(annotations,comments)

```
```{r}
tkn_df = df %>% unnest_tokens(word,comment)

word_counts = tkn_df %>% count(word, sort = TRUE)
word_counts
```


```{r}
tkn_df %>% 
  count(word, sort = TRUE) %>%
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)

```
```{r}
tkn_df = df %>% unnest_tokens(word,comment) %>% 
  #gsub('[A-Za-z', '', comment) %>%
  anti_join(stop_words) %>%
  filter(!grepl("newline_token",word)) %>% 
  filter(!grepl("\\d", word)) %>% 
  filter(!grepl("[_]",word))

totals = tkn_df %>% 
  count(word)

tkn_df = left_join(tkn_df, totals, by = "word") %>% filter(n > 10)
  
tkn_df %>% 
  count(word, sort = TRUE) %>%
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```

```{r}
library(SnowballC)
tkn_df = tkn_df %>% 
 mutate(stem = wordStem(word))

tkn_df %>% 
  count(stem, sort = TRUE) %>%
  top_n(20) %>% 
  mutate(stem = reorder(stem, n)) %>%
  ggplot(aes(n, stem)) +
  geom_col() +
  labs(y = NULL)
```

```{r}
sentiments = tkn_df %>% inner_join(get_sentiments("nrc")) #%>% filter(sentiment %in% c("positive","negative"))) %>% mutate(sentiment_score = ifelse(sentiment == "positive", 1, -1))

ggplot(sentiments, aes(x = sentiment, group = toxic)) +
  stat_count()+
  facet_wrap(~toxic)
```
```{r}
sentiments <- tkn_df %>%
  inner_join(get_sentiments("nrc")) %>%
  group_by(rev_id,toxic) %>% 
  count(sentiment) %>% 
  #count(rev_id, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(sentiments, aes(x = as.factor(toxic), y = sentiment, colour = as.factor(toxic))) + geom_point(alpha=.5) + geom_jitter()
```

```{r}
library(reshape2)
library(wordcloud)

tkn_df %>% 
 inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)

```

```{r}
split_df = initial_validation_split(df)

train_df = training(split_df)

validation_df = validation(split_df)

test_df = testing(split_df)
```

# Set up recipe for tokenizing words and getting tf-idf values
```{r}
rec <- recipe(toxic ~ comment, data = train_df) %>%
  step_tokenize(comment) %>%
  step_tokenfilter(comment, max_tokens = 1e3) %>%
  step_tfidf(comment) %>%
  step_normalize(all_predictors())

prep <- prep(rec)
bake <- bake(prep, new_data = NULL)

dim(bake)
```


# Fit a naive bayes model

```{r}
library(discrim)

nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")
```

```{r}
nb_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(nb_spec)
```

```{r}
nb_fit <- nb_wf %>%
  fit(data = train_df)
```

<!-- ## Cross-validation -->
<!-- ### Set up folds -->
<!-- ```{r} -->
<!-- folds <- vfold_cv(train_df, v = 10) -->
<!-- ``` -->

<!-- ### Resampling -->
<!-- ```{r} -->
<!-- nb_rs <- fit_resamples( -->
<!--   nb_wf, -->
<!--   folds, -->
<!--   control = control_resamples(save_pred = TRUE) -->
<!-- ) -->

<!-- ``` -->

<!-- ```{r} -->
<!-- nb_rs_metrics <- collect_metrics(nb_rs) -->
<!-- nb_rs_predictions <- collect_predictions(nb_rs) -->

<!-- nb_rs_metrics -->
<!-- ``` -->

```{r}
nb_pred <- augment(nb_fit, new_data = validation_df) %>% 
  mutate(correct = ifelse(.pred_class == toxic, 1, 0))

nb_pred %>%                # training set predictions
  roc_auc(truth = toxic, .pred_0)

nb_pred %>%                # training set predictions
  accuracy(truth = toxic, .pred_class)

```

```{r}
nb_pred %>%
  roc_curve(truth = toxic, .pred_0) %>%
  autoplot() +
  labs(
    color = NULL
  )
```

# Fit a lasso logistic regression model

```{r}
lasso_spec <- logistic_reg(penalty = 0.01, mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```

```{r}
lasso_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(lasso_spec)
```

```{r}
lasso_fit <- lasso_wf %>% 
  fit(data = train_df)
```

```{r}
lasso_pred <- augment(lasso_fit, new_data = validation_df) %>% 
  mutate(correct = ifelse(.pred_class == toxic, 1, 0))

lasso_pred %>%                # training set predictions
  roc_auc(truth = toxic, .pred_0)

lasso_pred %>%                # training set predictions
  accuracy(truth = toxic, .pred_class)

```

```{r}
folds <- vfold_cv(train_df, v = 5)

tune_spec <- logistic_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

lambda_grid <- grid_regular(penalty(), levels = 30)

tune_wf <- workflow() %>%
  add_recipe(rec) %>% 
  add_model(tune_spec)


start_time <- Sys.time()
tune_rs <- tune_grid(
  tune_wf,
  folds,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE)
)
end_time <- Sys.time()
time.take = end_time - start_time

collect_metrics(tune_rs)

```

```{r}
autoplot(tune_rs) +
  labs(
    title = "Lasso model performance across regularization penalties",
    subtitle = "Performance metrics can be used to identity the best penalty"
  )
```

```{r}
chosen_auc <- tune_rs %>%
  select_by_one_std_err(metric = "roc_auc", -penalty)

final_lasso <- finalize_workflow(tune_wf, chosen_auc)

lasso_fit <- fit(final_lasso, train_df)
```


```{r}
lasso_pred <- augment(lasso_fit, new_data = test_df) %>% 
  mutate(correct = ifelse(.pred_class == toxic, 1, 0))

lasso_pred %>%                # training set predictions
  roc_auc(truth = toxic, .pred_0)

lasso_pred %>%                # training set predictions
  accuracy(truth = toxic, .pred_class)

```
```{r}
nb_pred <- augment(nb_fit, new_data = test_df) %>% 
  mutate(correct = ifelse(.pred_class == toxic, 1, 0))

nb_pred %>%                # training set predictions
  roc_auc(truth = toxic, .pred_0)

nb_pred %>%                # training set predictions
  accuracy(truth = toxic, .pred_class)

```


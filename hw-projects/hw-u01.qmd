---
title: "Homework U01: NIJ Recidivism Challenge"
format: html
editor: source
---

![](../imgs/Angola-prison1.jpg)

::: {.callout-caution icon="false"}
## Learning Objectives

Our learning objectives for this homework are:

- Apply the fundamental machine learning workflow from start to finish
- Use data munging skills to conduct feature engineering for machine learning
- Gain hands-on experience with building and applying logistic regression models
- Use model visualizations to gain insight into the dynamics of criminality and rehabilitation
:::

In this homework, we'll take on the National Institute of Justice’s (NIJ) [Recidivism Forecasting Challenge](https://nij.ojp.gov/funding/recidivism-forecasting-challenge) that challenged data scientists to forecast the probability that parolees would recidivate within one, two, or three years.

Here's how the NIJ described it:

> The Challenge uses data on roughly 26,000 individuals from the State of Georgia released from Georgia prisons on discretionary parole to the custody of the Georgia Department of Community Supervision (GDCS) for the purpose of post-incarceration supervision between January 1, 2013 and December 31, 2015. 

> The Challenge aims to improve the ability to forecast recidivism using person- and place-based variables with the goal of improving outcomes for those serving a community supervision sentence. We hope through the Challenge to encourage discussion on the topics of reentry, bias/fairness, measurement, and algorithm advancement.

 With one of the highest incarceration rates in the world, U.S. prisons house a vast population of inmates, many of whom are serving time for non-violent offenses. This system, characterized by overcrowding and limited access to effective rehabilitation programs, tends to create a cycle of recidivism rather than offering a path to societal reintegration. The lack of rehabilitative focus not only challenges the prospects of successful reentry for former inmates but also raises significant questions about the broader goals and effectiveness of the penal system in the United States. This systemic issue underscores the need for a paradigm shift towards more rehabilitative and restorative approaches in addressing crime and punishment.

The parole system in Georgia is designed as a means of reintegrating offenders back into society under supervised release, after they have served a portion of their prison sentence. The decision to grant parole is based on several factors, including the nature of the crime, the inmate's behavior while incarcerated, their rehabilitation efforts, and the potential risk to public safety.

Once granted parole, the individual is released into the community but must comply with specific conditions set by the parole board. These conditions often include regular meetings with a parole officer, drug testing, employment requirements, and sometimes restrictions on travel and association. Violating these conditions can result in the revocation of parole and a return to prison.

**Here, we will apply logistic regression to this dataset to predict recidivism among people formerly incarcerated in the Georgia prison system.**


## The data

The data for this homework comes from the National Institute for Justice Georgia's “Recidivism Forecasting Challenge.” 


> The Challenge uses data on roughly 26,000 individuals from the State of Georgia released from Georgia prisons on discretionary parole to the custody of the Georgia Department of Community Supervision (GDCS) for the purpose of post-incarceration supervision between January 1, 2013 and December 31, 2015. 


Before you begin, please take note of the data's terms of use:

> **Privacy and Confidentiality of Data**
>
> Any intentional identification of a individuals or unauthorized disclosure of his or her confidential information violates the promise of confidentiality given to the providers of the information. Therefore, users of data agree:  
>
> - To use these datasets solely for research or statistical purposes and not for investigation of specific individuals
> - To make no use of the identity of any individual discovered inadvertently, and to advise NIJ of any such discovery (NIJRecidivismChallenge@usdoj.gov)

I've downloaded the data and performed some initial data munging to get you started (copy and paste this to load the data).

```{r}
#| message: false
library(tidyverse)
library(tidymodels)

to_numeric <- function(x){
  suppressWarnings(y <- as.numeric(x))
  if_else(is.na(y), max(y+1, na.rm=T), y)
}

nij <- read_csv("data/NIJ_s_Recidivism_Challenge_Full_Dataset_20240112.csv") %>% 
  mutate(recidivated = factor(if_else(Recidivism_Within_3years, "Yes", "No"))) %>% 
  mutate_at(vars(matches("_Arrest_|_Conviction_|Program_Attendances")), to_numeric) %>%
  mutate_if(is.character, factor) %>% 
  select(-contains("Recidivism"), -Gang_Affiliated, -Training_Sample, -Supervision_Level_First, - Residence_PUMA) %>% 
  mutate_if(is.logical, factor) %>% 
  drop_na(where(is.numeric))

```


## Exercise 1

We want to predict which parolees will recidivate within 3 years of release. First, let's see how many recidivists vs non-recidivists we have in our dataset.

::: {.callout-note icon="false"}
## Exercise 1

First, load the data using the code show nabove. Then, examine the balance of the classes (1 = recidivated, 0 = no recidivation) coded by the `recidivated` variable. Calculate the proportion who recidivated.

✅ **Answer Check**: Report the proportion who recidivated.
:::

## Exercise 2

Before we make a model, let's do some further exploratory data analysis. We'll take a look at the Supervision Risk score (`Supervision_Risk_Score_First`). This is a composite score based on a formula considering inmates' prior offenses, age, behavior in prison, and other variables.

::: {.callout-note icon="false"}
## Exercise 2
Make a line-plot of the proportion of parolees who recidivated across supervision risk scores, with the proportion on the y-axis and risk scores alon the x-axis.
:::


## Exercise 3

Let's begin our machine-learning workflow! 

::: {.callout-note icon="false"}
## Exercise 3
Split the data into training (80%) and testing (20%) splits. Since we won't use it for prediction, drop the `ID` column before making these splits. Make sure to stratify the train/test splits by our outcome to ensure a representative class balance for our predictions. Call your training and testing splits `train` and `test`

✅ **Answer Check**: How many rows are in your training set?
:::

## Exercise 4

We'll begin with a simple model using a single predictor: The initial supervision risk assessment score assigned upon release (`Supervision_Risk_Score_First`). This will let us see how diagnostic the existing parole methodology is.

::: {.callout-note icon="false"}
## Exercise 4

Create a workflow and fit a model predicting `recidivated` from `Supervision_Risk_Score_First`. You can add the formula directly to the workflow.

✅ **Answer Check**: Write the formula you used.

:::


## Exercise 5

Let's examine this model in a few ways. 

::: {.callout-note icon="false"}
## Exercise 5

1. First, `tidy()` the model and examine the coefficient for `Supervision_Risk_Score_First`. How would you interpret the coefficient? Calculate the odds ratio of recidivating for a parolee with a score of 2 compared to a parolee with a score of 9.
2. Then, calculate the probability of recidivating for parolees with a score of 2 versus 9:  
    - using `augment()` or `predict()`
    - manually using `plogis()`
    
✅ **Answer Check**: Report the value you calculated using `plogis()`.
:::


## Exercise 6

Consider what you should make of the accuracy score you observe in light of the balance of recidivism you calculated in Exercise 1.

::: {.callout-note icon="false"}
## Exercise 6

Calculate the expected accuracy of a "classifier" that operates by guessing based on the overall probability of recidivism. How does such a "null" model compare to our model in terms of accuracy?
:::

::: callout-tip
## "Null" models
We'll sometimes refer to a "null" model as a point of comparison. Roughly, kind of like the standard null hypothesis, a null model is one in which "nothing is going on." Usually, it's a model that doesn't depend on any predictors, that just predicts the same thing for every instance. If we can't beat the null model, then we really aren't doing so hot!
:::


## Exercise 7

Accuracy can be misleading when there are uneven numbers of positive and negative cases or classes in our data. To better evaluate our model, we need to employ a metric other than simple accuracy. One option for evaluating classifiers in cases with unbalanced outcome classes is **Balanced Accuracy**. 

Balanced accuracy is the average of the model's true positive rate (TPR) and its true negative rate (TNR). Unlike simple accuracy, it is "balanced" because it gives equal weight to positive and negative cases.

::: {.callout-note icon="false"}
## Exercise 7
Calculate the performance of the model on the test data. Since the classes are not perfectly balanced, calculate both accuracy and balanced accuracy. In addition, calculate the `sensitivity` (TPR) and `specificity` (TNR) of the model, making sure to use the `event_level = "second"` flag. 

Briefly consider how the TPR and TNR might influence the use of such a model.

✅ **Answer Check**: Report the TPR and TNR. Briefly describe your thoughts about the question above.
:::


::: callout-tip
The `metric_set()` function can help you do this without coding things up manually.

:::


## Exercise 8

Can we do better with machine learning than the existing risk score? 

Let's build a model that brings to bear all of the other data we have available from this challenge dataset.

::: {.callout-note icon="false"}
## Exercise 8

Build a recipe to do a few things:

1. Remove `Supervision_Risk_Score_First` variable (`step_rm()`) from the data. We want to ignore this for our machine learning model, since this is what we are going to compare against.
2. Replace any missing values for categorical variables with a new "unknown" factor level.
:::


## Exercise 9

::: {.callout-note icon="false"}
## Exercise 9
Fit and calculate the performance on the test data for a model using this recipe. Compare this to the model using just the traditional supervision risk score assessment in terms of accuracy, balanced accuracy, and TPR/TNR.

✅ **Answer Check**: Which is better in terms of balanced accuracy?
:::


## Exercise 10

Let's dive in to interpreting the model a bit, by considering two potentially important variables that might be somewhat under a parolee or the parole system and programming control: employment and program attendance. 

We'll examine the relative importance of these two predictors.

::: {.callout-note icon="false"}
## Exercise 10

- `tidy()` the model and filter the result to display just the coefficients for these two predictors. Look at the codebook to ensure you are interpeting them correctly in terms of their original predictors. Can you tell which is more important?
- Interpeting the relative importance of coefficients is easier with standardized predictors. Re-fit the model adding a `step_normalize()` to the recipe and then compare thes coefficients again.
- Make two line plots to explore the proportion of recidivists across `Program_Attendances` and a binned version of `Percent_Days_Employed` (use 10 bins). Put proportion of recidivists on the y-axis and the predictor on the x-axis. 

✅ **Answer Check**:  Compare the plots to the relative importance you determined based on the model coefficients. Do these two ways of exploring this correspond well? do these two ways of exploring this correspond well? and briefly share your thoughts.

:::

## Exercise 11

What does it all mean?

Do employement and/or program attendance reduce the risk of recidivism? Or do other factors that reduce the risk of recidivism support consistency in employment and attendance (e.g. relationships, social support, financial safety)? 

More generally, how do you think should parole decisions should incorporate these kinds of predictive models to support both community safety but also justice and fairness? What does this level of accuracy support? What if you imagine we could make these predictions more accurately (e.g. 95% or 99%)?

::: {.callout-note icon="false"}
## Exercise 11
✅ **Answer Check**: Reflect on the questions above and write 1-2 paragraphs discussing your thoughts.
:::


# Wrapping up

When you are finished, knit your Quarto document to a PDF file.

::: {.callout-important icon="false"}
**MAKE SURE YOU LOOK OVER THIS FILE CAREFULLY BEFORE SUBMITTING**
:::

When you are sure it looks good, submit it on Canvas in the appropriate assignment page.


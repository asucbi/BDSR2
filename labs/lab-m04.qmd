---
title: "Module 4 Lab: Model Evaluation and Comparison"
author: "Ben Falandays"
bibliography: ../references.bib
editor: source
---

# Introduction

::: {.callout-caution icon="false"}
## Learning Objectives

Our learning objectives for this module are:

-   Understand basic metrics of model fit, including accuracy, precision, recall, and AIC
-   Use metrics to compare models
-   Understand how model evaluation and comparison is used to select parameters, tune models, and avoid overfitting.
:::

In this course, you are going to learn how to build a variety of different machine-learning models--you've already learned linear and logistic regression, as well as the Naive Bayes algorithm, and there will be many more to come. As a data scientist, your goal in every situation is to select an _appropriate_ model for the task at hand. This includes not only figuring out what _type_ of model to use, but also what _variables_ to put in your model as predictors, as well as how to set any model _parameters_.

In order to make these choices, we first need some ways to assess how appropriate a given model is--we need to use performance metrics. If we're using a classification model, a simple performance metric is _accuracy_--how often your model correctly classifies new data from the test set. But metrics can get more complicated than that, as we'll see in this module.

```{r}
#| message = FALSE
library(tidymodels)
library(tidyverse)
library(MASS)
```

# Data

In this module, we will explore a common situation data scientists encounter, in which they have numerous _potential_ predictor variables, and need to decide which variables to include in a model. We could simply throw all of the data we have into our model and hope for the best, but we often prefer to have the _simplest_ model that captures the data. There are several reasons for this preference, including that more complex models (having more predictors or other parameters to fit) can require much more time and/or data to compute, and that overfitting becomes a bigger risk with more complex models, especially if the training data is small.

To illustrate this point, we will use simulated data. This will let us examine a situation in which we _already know_ which predictor variables are useful, and see how metrics could be used to guide our modeling choices in future situations where we _don't know_ which variables matter.

We will simulate data appropriate for a logistic regression model, which you have been introduced to in earlier modules. First, we generate 5 potential predictor variables, `x1`-`x5`, by drawing samples from a normal distribution with. We will imagine a situation in which we have limited data--`n_observations` is set to just 50.

```{r}
set.seed(123)

n_observations = 50

df_sim = tibble(x1 = rnorm(n_observations, mean = 0, sd = 2),
                x2 = rnorm(n_observations, mean = 0, sd = 2),
                x3 = rnorm(n_observations, mean = 0, sd = 2),
                x4 = rnorm(n_observations, mean = 0, sd = 2),
                x5 = rnorm(n_observations, mean = 0, sd = 2)
                )
```

In logistic regression, the response variable (below called `z`) represents the log-odds (logit) of the probability of success. We will make `z` a function of the first two predictor variables only, `x1` and `x2`, while `x3`-`x5` will be irrelevant. 
```{r}
df_sim = df_sim %>% 
  mutate(z = .5 + 2*x1 + 3*x2)
```

Because the response variable is in log-odds units, we next convert it to a standard probability
```{r}
df_sim = df_sim %>% 
  mutate(pr = 1/(1+exp(-z)))
```

Finally, we can simulate a binary outcome variable, `Class`, by sampling from a binomial distribution using the `rbinom()` function.
```{r}
df_sim = df_sim %>%
  mutate(Class = rbinom(n=nrow(df_sim),size=1,prob=pr)) 

df_sim$Class = factor(df_sim$Class)
```

# Data Exploration

::: {.callout-note icon="false"}
## Exercise 1
Let's start by plotting the data a bit to understand what we're dealing with. First, make a scatterplot of variables `x1` and `x2` on the x- and y-axes of the plot, and points colored by `Class`. 
:::
```{r}
ggplot(df_sim, aes(x = x1, y = x2, color = Class)) + geom_point()
```
Next, we'll plot the outcome variable, `Class`, as a function of `x1` and `x2` separately, and fit a logistic curve to the data.
```{r}
ggplot(df_sim, aes(x = x1, y = as.numeric(Class)-1)) + geom_point(alpha = .1) +
    geom_smooth(method = 'glm', method.args = list(family = "binomial"))

ggplot(df_sim, aes(x = x2, y = as.numeric(Class)-1)) + geom_point(alpha = .1) +
    geom_smooth(method = 'glm', method.args = list(family = "binomial"))
```
::: {.callout-note icon="false"}
## Exercise 2

Make the same plot as above, for variables `x3`, `x4` and `x5`. Explain how the logistic curves are different from above, and why.
:::
```{r}
ggplot(df_sim, aes(x = x3, y = as.numeric(Class)-1)) + geom_point(alpha = .1) +
    geom_smooth(method = 'glm', method.args = list(family = "binomial"))

ggplot(df_sim, aes(x = x4, y = as.numeric(Class)-1)) + geom_point(alpha = .1) +
    geom_smooth(method = 'glm', method.args = list(family = "binomial"))

ggplot(df_sim, aes(x = x5, y = as.numeric(Class)-1)) + geom_point(alpha = .1) +
    geom_smooth(method = 'glm', method.args = list(family = "binomial"))
```
::: {.callout-note icon="false"}
## Exercise 3

Using the normal procedure, split df_sim into testing and training sets. Then, build a model specification for a logistic regression model, with `mode` set to "classification", `engine` set to "glm", and `penalty` and `mixture` set to `NULL`.
:::
```{r}
df_split <- initial_split(df_sim)
df_train <- training(df_split)
df_test <- testing(df_split)
```

```{r}
log_spec <- logistic_reg(
  mode = "classification",
  engine = "glm",
  penalty = NULL,
  mixture = NULL
)
```

::: {.callout-note icon="false"}
## Exercise 4

Next, build two different formulas for logistic regression models. The first, `f_all` is an overly-complex formula containing all of variables `x1`-`x5`, as well as their interaction terms. Build a second formula, `f_real`, containing just two terms for `x1` and `x2`. Then, fit two models to the training data using each formula
:::
```{r}
f_all <- formula(Class ~ x1 * x2 * x3 * x4 * x5)

f_real <- formula(Class ~ x1 + x2)
```

```{r}
m_real <- workflow() %>% 
  add_model(log_spec) %>% 
  add_formula(f_real) %>% 
  fit(data = df_train)

m_all <- workflow() %>% 
  add_model(log_spec) %>% 
  add_formula(f_all) %>% 
  fit(data = df_train) 
```
We will create a metric set, `multi_metric`, which will pull the accuracy, precision, recall, and F-score for each model.

```{r}
multi_metric <- metric_set(accuracy,precision,recall)
```

::: {.callout-note icon="false"}
## Exercise 5

As you've done previously, augment the training data with predictions from the model, and then extract the metrics.
:::
```{r}
m_real %>% augment(df_train) %>% multi_metric(truth=Class, estimate=.pred_class)
```

Another useful model metric is called Aikaike Information Criterion, or AIC. AIC is influenced by model fit, but also by model complexity. As such, this metric can help us to choose the simplest model that captures the data effectively. Models with a worse fit, or with more terms than necessary, will make the AIC go up, so lower scores are better.

To get the AIC of the model, we can extract the model fit using `extract_fit_engine()` and pipe it to the `AIC()` command.
```{r}
m_real %>% extract_fit_engine() %>% AIC()
```

::: {.callout-note icon="false"}
## Exercise 6

Now, repeat the steps above for the more complex model, `m_all`
:::

```{r}
m_all %>% augment(df_train) %>% multi_metric(truth=Class, estimate=.pred_class)

```

```{r}
m_all %>% extract_fit_engine() %>% AIC()
```

From fitting the models to the training data, we can see that `m_real` (the model with only the real predictors, `x1` and `x2`) does very well. The model with every possible predictor, `m_all` does _perfectly_, but we can see that its AIC value is much higher--64, relative to 29 in the simpler model. This suggests that `m_all` is overly complex, and perhaps our perfect performance is actually an indication that our model is overfit! We can confirm this by evaluating model performance on the test data.

::: {.callout-note icon="false"}
## Exercise 7

Repeat the model evaluation for `m_all` and `m_real`, but this time on the testing data.
:::
```{r}
m_all %>% augment(df_test) %>% multi_metric(truth=Class, estimate=.pred_class)

```
The much more complex model, which did perfectly on the training set, only gets around 38% accuracy on the test data!

```{r}
m_real %>% augment(df_test) %>% multi_metric(truth=Class, estimate=.pred_class)

```

On the other hand, the model with only the real predictors, `m_real`, still does quite well on the training data. 

# Takeaway Lesson

This exercise illustrates the importance of thinking carefully about model building. If we don't know which variables are useful for predicting an outcome, and we simply throw everything into our model, we can run into problems when there is limited data. In this example, the overly-complex model does perfectly on the training set, but we can see how this is a result of overfitting--the same model does very poorly on the test set. Meanwhile, the simpler model has slightly worse accuracy on the training set, but turns out to be the best model overall. 

In this situation, we knew in advance what the better model would be, because we used simulated data that came from a known function. Of course, in most situations that won't be true. As such, you'll need to use performance metrics to evaluate and compare models as you build them. 

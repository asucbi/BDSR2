---
title: "Module 4 Lab: Human Activity Recognition w/ KNN & Cross-Validation"
author: "Ben Falandays"
bibliography: ../references.bib
editor: source
---

::: {.callout-caution icon="false"}
## Learning Objectives

Our learning objectives for this module are:

-   Understand the bias-variance tradeoff (accuracy vs overfitting)
-   Understand the k-nearest neighbors algorithm for classification and implement it with tidymodels
-   Use cross-validation to tune the value of k
:::

Our cellphones and smartwatches record a lot of different kinds of data. There's our search history and web traffic, of course, but also GPS, audio data, and a variety of sensors, including a gyroscope, accelerometer, and gravity sensor. In this module, we'll see how that sensor data can be used to predict what a user is doing during some interval of time--whether they're sitting down, riding a bike, driving a car, etc. This type of analysis could be useful for a tech company to figure out your interests and serve up targeted ads, or to deliver content/functionality at just the right moment (e.g. maybe you want your maps interface to change automatically depending upon whether you are biking, driving, or walking).

The dataset for this module contains summarized data (i.e. means, maxs, mins, etc) from the accelerometer, gyroscope, and gravity sensor of a smartwatch while participants performed various actions. We will use the k-nearest neighbors (knn) algorithm to classify the activity being performed. Then, we will use k-fold cross-validation to tune the parameters of our knn model in order to get the highest classification accuracy, while avoiding overfitting. 

::: column-margin
Here is where we might bore students with some minutia or a formula like $\hat{y} = \alpha + \beta x$.
:::

## Library and data loading

```{r}
#| message = FALSE
library(tidyverse)
library(tidymodels)
library(broom)
library(themis)

df = rbind(read_csv('./data/OriginalProcessedData.csv'), read_csv('./data/NewProcessedData.csv'))

colnames(df) <- gsub("~","", colnames(df))

df$Activity = factor(df$Activity)
```

# Data visualization and exploration

::: {.callout-note icon="false"}
## Exercise 1

First, make a bar plot showing the number of observations of each type of `Activity`.
:::
```{r}

ggplot(df, aes(x = Activity, fill = Activity))+geom_bar()

```
One thing to notice is that we have substantial "class imbalance," meaning there are many observations of some activities--such as cycling and football--and far fewer observations of other activities, such as jogging and pushups. Class imbalance can cause problems for our model, because the training data is driven much more strongly by only a subset of activities. 

To deal with this problem, we can use resampling techniques. We can either "upsample," resample rows from classes with fewer observations, or "downsample," excluding rows from classes with more observations. 

In this case, we will try upsampling using the `step_upsample()` function. We set `over_ratio` to .5, meaning we will oversample the classes with fewer observations until they have 50% as many observations as the class with the most observations. The plot below shows what the distribution of activities will look like after this resampling step.

```{r}
recipe(Activity ~ ., df) %>% 
  step_upsample(Activity, over_ratio = .5) %>% 
  prep() %>%
  bake(new_data = NULL) %>%
  ggplot(aes(x = Activity, fill = Activity))+geom_bar() +
  geom_bar()
```

Next, we'll explore the data a bit to see what it looks like while doing different activities. Notice that there are a LOT of variables in the dataset. First, we'll just grab any two variables and make a scatterplot, with points colored by `Activity`. I chose the mean of the accelerometer data in the X- and Y-axes, called `ACCELEROMETER_X_mean` and `ACCELEROMETER_Y_mean`

```{r}
ggplot(df, aes(x = ACCELEROMETER_X_mean, y = ACCELEROMETER_Y_mean, color = as.factor(Activity))) + geom_point()

```
We can see that the data is somewhat clustered. For example, there is a cloud of points associated with swimming, another for cycling and for football. But overall, it's pretty messy, and it would probably be hard to accurately predict the activity from this data.

::: {.callout-note icon="false"}
## Exercise 2

Next, make the same plot as above using the X- and Y-axis means of the gravity sensor, and another for the gyroscope data. Do you notice any differences? Which pair of variables do you think would be best for predicting `Activity`?
:::
```{r}
ggplot(df, aes(x = GRAVITY_X_mean, y = GRAVITY_Y_mean, color = as.factor(Activity))) + geom_point()

```
Our dataset has a lot of different (potential) predictor variables--112, to be precise. As such, we can think of each row as a point in a 112-dimensional space. Above, we plotted the data in 3 different, 2-dimensional slices of that 112-D space. We can see that the datapoints appear more discriminable when we look at some dimensions rather than others. But how well could we tell the groups apart if we could see in all 112 dimensions at once?

The model we'll use today, the k-nearest neighbors (KNN) algorithm, is based on the distance between points. For any two points, we can measure their distance from each other in each of the 112 dimensions. KNN assumes that "birds of a feather flock together"--meaning that observations from different `Activity` categories will cluster in distinct _regions_ of this 112-dimensional space. When we encounter a new, unlabeled point and want to predict it's class, KNN simply checks the class labels for the _k_ nearest other points (in the training set) and takes their labels as "votes" to determine the label of the new point. 

::: callout-tip
_k_ (called `neighbors` in our model spec) should always be an odd number, to avoid ties in voting.
:::

# Data splitting

::: {.callout-note icon="false"}
## Exercise 3

Using tidymodels, split the data into testing and training splits called `df_test` and `df_train`, respectively.
:::

```{r}
df_split = initial_split(df)
df_train <- training(df_split)
df_test <- testing(df_split)
```

# Building a KNN model

::: {.callout-note icon="false"}
## Exercise 4

Next, make a model specification using tidymodels, for a k-nearest neighbors model. The function is called `nearest_neighbor()`. The `mode` should be set to *"classification"*, the `engine` to *"kknn"*, and let's set `neighbors` to *5*, to start.
:::

```{r}
knn_spec <- nearest_neighbor(
  mode = "classification",
  engine = "kknn",
  neighbors = 5
)
```

::: {.callout-note icon="false"}
## Exercise 5

Below, I supply you with a recipe, `rec`, that deals with upsampling. Finish building a workflow called `knn_wf`, by adding your model specification, as well as the upsampling recipe.
:::
```{r}
rec <- recipe(Activity ~ ., df) %>% 
  step_upsample(Activity, over_ratio = .5) 

knn_wf <- workflow() %>% 
  add_model(knn_spec) %>% 
  add_recipe(rec)
```

# Fitting the model

::: {.callout-note icon="false"}
## Exercise 6
Next, fit the model to the training data.
:::

```{r}
knn_fit <-
  knn_wf %>% 
  fit(data = df_train)
```
Keep in mind that the model we have built so far is just a starting point--we will need to tune the model in order to find the best parameter(s). For the k-nearest neighbors algorithm, there is just one model parameter: the value `k` (the number of neighbors considered). To find the best value for `k`, we will need to run the model with different values of `k`, and collect some metric of performance. But importantly, we don't want to use our testing data just yet to evaluate our models--the test set must be saved for the final model. 

::: {.callout-note icon="false"}
## Exercise 7

Explain WHY we don't want to evaluate model performance on the test set while we are tuning our model.
:::

So how are we supposed to evaluate model performance for the purpose of parameter tuning, without using the test set? That's where *cross-validation* comes in. We divide the training data into _n_ subsets, set aside one subset as the current validation data, and train the model on the remaining subsets. We then repeat that procedure _n_ times, each time leaving out a different subset of the data. This amounts to *resampling* the training set to use it as a preliminary test set. With this approach, we can test out different values of _k_ (`neighbors` in the KNN) and estimate how each will perform, before tapping into our precious test set.


# Model evaluation with cross-validation

::: {.callout-note icon="false"}
## Exercise 8
Use the `vfold_cv()` function from tidymodels to divide `df_train` into 5 folds. Then, use the `fit_resamples()` function to estimate the performance of our initial KNN model.
:::
```{r}
folds <- vfold_cv(df_train, v = 5)

knn_rs <- fit_resamples(
  knn_wf,
  folds,
  control = control_resamples(save_pred = TRUE)
)

collect_metrics(knn_rs)
```

# Model tuning

Next, we will tune the model to find the best value of _k_ (`neighbors`). We essentially just repeat the process above for different values of _k_, but we'll do it in a nice tidymodels way.

::: {.callout-note icon="false"}
## Exercise 9
Create a new model specification, like `knn_spec` above, but set `neighbors = tune()`.
:::
```{r}
tune_spec <- nearest_neighbor(
  mode = "classification",
  engine = "kknn",
  neighbors = tune()
)
```

::: {.callout-note icon="false"}
## Exercise 10
Make a tibble called `neighbor_grid`, containing a list of values for _k_ (`neighbors`) to search through for your model tuning. These should be odd numbers (to avoid voting ties) with a minimum of 1 and a maximum of 19.

::: {.callout-note icon="false" collapse="true"}
## Hint
Use the `seq()` function to create a sequence of odd numbers from 1-19.
:::
:::
```{r}
neighbor_grid <- tibble(neighbors = seq(1,100,by=4)) 
```

::: {.callout-note icon="false"}
## Exercise 11

Next, build a workflow, exactly as you did with `knn_wf` above.
:::
```{r}
tune_wf <- workflow() %>% 
  add_model(tune_spec) %>% 
  add_recipe(rec)
  
```

::: {.callout-note icon="false"}
## Exercise 12

Finally, use `tune_grid()` to tune your model parameters. The function arguments are the same as `fit_resamples()`, except we add `grid = neighbor_grid` to supply the values of _k_ `neighbors` to try.
:::
```{r}
tune_rs <- tune_grid(
  tune_wf,
  folds,
  grid = neighbor_grid,
  control = control_resamples(save_pred = TRUE)
)

```

We can use `autoplot()` to quickly visualize how the model performed at each value of _k_.
```{r}
autoplot(tune_rs)
```

Finally, we need to choose a value for _k_ for our final model, which we will evaluate on the test set. Then we fit the model with that parameter, and evaluate it's performance on the test set
```{r}
chosen_n <- tune_rs %>% select_by_one_std_err(metric = "roc_auc", neighbors)

final_knn <- finalize_workflow(tune_wf, chosen_n)

knn_fit <- fit(final_knn, df_train)

knn_fit %>% 
  augment(df_test) %>% 
  metrics(Activity, .pred_class)
```

Our model gets an accuracy of around 84%. That seems pretty good, given that there are 12 classes. But what would be chance performance here? We need to compare our final model with a null model. 

If we were guessing randomly, we would be right 1/12th of the time (8.3% accuracy). But that's not the most fair point of comparison, because an uninformed model could still do better than that. Consider that our data is not uniformly distributed across activities. This means that some activities account for _more_ than 1/12th of the data, and others for less. The most frequent activity is football, which accounts for over 19% of the data. This means that an uninformed model could simply guess 'football' for every data point, and it would be correct 19% of the time. This is a more reasonable null model. 

We can fit a null model to our training data and evaluate its performance on the test set.

```{r}
null_spec <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")
  
null_wf <- workflow() %>% 
  add_model(null_spec) %>% 
  add_formula(Activity ~ .)

null_fit <-
  null_wf %>% 
  fit(data = df_train)

null_fit %>% 
  augment(df_test) %>% 
  metrics(Activity, .pred_class)
```
We can see that this null model has accuracy of .182. Note that this is the exact value you'd get if you guessed 'football' for every datapoint in the test set, since 18.2% of them _actually are_ football. To see this for yourself, get a quick count of the frequency of each `Activity` in `df_test`

```{r}
df_test %>% group_by(Activity) %>% summarise(N = n()) %>% arrange(desc(N))
```
We can see that football is observed 296 times in the test set. If we calculate what _proportion_ of the test set that is, we'll see that it's .182--same as the accuracy of our null model.
```{r}
296/nrow(df_test)
```



---
title: "Module 4 Lab: Model Evaluation and Comparison"
author: "Ben Falandays"
bibliography: ../references.bib
editor: source
---

# Introduction

::: {.callout-caution icon="false"}
## Learning Objectives

Our learning objectives for this module are:

-   Understand basic metrics of model fit, including accuracy, precision, recall, and AIC
-   Understand how model evaluation and comparison is used to select and tune models
:::

![Please don't eat me, Jane!](../imgs/babycow.jpeg){width=75%}

Imagine that Jane has made a moral commitment to not eating veal anymore, despite the fact that veal parmigiana is her favorite dish. She sits down at her favorite Italian restaurant and peruses the menu. Her eyes flit back and forth between her old favorite, veal parmigiana, and her new replacement, chicken parmigiana. She wants to adhere to her new moral code, but the veal is tempting. Just as her eyes happen to have settled on the veal for about 1 s, suddenly the waiter walks up and asks what she would like to order. If the waiter had shown up a second or two later, she might have managed to settle her eyes, and her mind, back on the chicken. But, in that moment, her decision is prompted and she caves, ordering the veal.

The dataset for this lab comes from an experiment in which we tested the idea that our decisions could be swayed by the timing of when we are asked for a response. In the experiment, participants heard moral questions, such as "Is murder ever justifiable?", and non-moral questions, such as "Is Denmark larger than Sweden?" For each question, there were two possible response options, and participants were instructed to click on their preferred choice. 

Unbeknownst to participants, the experiment software was trying to get them to choose one of the two options on each trial. For each question, one of the two responses was randomly designated as the "target" response, and the experimenters used an eye tracker to record the amount of time the participant spent looking at the target response, versus the other response. Only once the participant spent at least 750ms looking at the target option were they prompted to respond. The experimenters hypothesized that participants would be more likely to choose the target when (1) they spent more time looking at the target, relative to the other response, (2) the target was the *last* thing they looked at before being prompted to respond.

The variables in this dataset include:

- `subj_id`: a unique identifier for each participant
- `trialnum`: an identifier for which trial each row corresponds to (in terms of order within the study)
- `Q_type`: whether the question was a moral or non-moral one
- `targ_loc`: whether the "target" response was on the left or right side of the screen
- `choice_loc`: whether the participant chose the response on the left or right side of the screen
- `RT`: reaction time
- `left_fixTime_ms`: time (in milliseconds) spent looking at the response option on the left side of the screen
- `right_fixTime_ms`: time (in milliseconds) spent looking at the response option on the right side of the screen
- `Q_id`: a unique identifier for each question
- `lastfix`: whether the participant looked at the left or right response option last (just before they were prompted to respond)
- `Q_text`: the text of question they heard
- `option1_text`: the text of the first response option
- `option2_text`: the text of the second response option

```{r}
#| message = FALSE
library(tidymodels)
library(tidyverse)
load("~/Documents/Github/BDSR2/data/moralDecisions.Rdata") 
```

```{r}
head(moralDecisions)
```


# Exercise 1
::: {.callout-note icon="false"}
## Exercise 1

Our first goal will be to make a plot that shows...

- On the y-axis: whether or not the participant chose the target
- On the x-axis: the time spent fixating the target, minus the time spent fixating the other option

To do this, we will first need to make some new variables:

1. Make a new variable called `chose_targ`, which will be equal to 1 if the participant chose the target, or 0 otherwise
  - You will need to use `targ_loc` and `choice_loc` to make this variable
2. Make two new variables, `targ_fixTime` and `nontarg_fixTime`, giving the time spent fixating the target or non-target, respectively. 
  - You will need to use `left_fixTime_ms`, `right_fixTime_ms`, and `targ_loc`
3. Make a variable called `targ_fix_bias`, giving the time spent fixating the target *minus* the time spent fixating the non-target
4. Make a variable called `lastfix_item`, which identifies whether the last fixation before prompting was to the target or non-target
  - You will need to use `lastfix` and `targ_loc`
  
Once you have all the variables, set up your initial plot. 

::: {.callout-tip}
You may want to jitter your points and adjust the alpha level to make all of the datapoints visible.

:::
:::

```{r}

```

# Exercise 2
::: {.callout-note icon="false"}
## Exercise 2

So far, you've learned about three different models: linear regression, and naive Bayes, logistic regression. Which model(s) do you think would be appropriate for this dataset?

Once you've answered that question, use `geom_smooth()` to add smoothed conditional means to your plot. Set the `method` and `method.args` accordingly, depending upon which model type you think is most appropriate. Additionally, use different colors for each level of `Q_type` and different linetypes for each level of `lastfix_item`.
:::

```{r}

```

# Exercise 3
::: {.callout-note icon="false"}
## Exercise 3

Make your train/test split of the data. Rather than using `initial_split()`, use `group_initial_split()` with `group` set to `subj_id`. This will make sure that data from an individual subject stays together, and goes into *either* the test or the train split.
:::
```{r}

```

# Exercise 4
::: {.callout-note icon="false"}
## Exercise 4

Build one or more model specifications and workflows, depending upon which model(s) you think could be applied to this type of data. Our goal will be to predict whether, on each trial, the participant chose the target or the non-target.
:::

```{r}

```

# Exercise 5
::: {.callout-note icon="false"}
## Exercise 5

Fit one or more very simple model(s) to the training data, predicting `chose_targ` from just `lastfix_item`. Assess the performance on the training data by getting the balanced accuracy, F-measure, and ROC-AUC.
:::

```{r}

```

# Exercise 6
::: {.callout-note icon="false"}
## Exercise 6

Fit one or more complex model(s) to the training data, predicting `chose_targ` from just `lastfix_item`, `targ_fix_bias` and `Q_type`. Assess the performance on the training data by getting the balanced accuracy, F-measure, and ROC-AUC.
:::

```{r}

```

# Exercise 7
::: {.callout-note icon="false"}
## Exercise 7

Based on your result above, which model do you think will do best on the test set? Evaluate all of your models on the test set and see if your expectations were correct.
:::

```{r}

```

# Exercise 8
::: {.callout-note icon="false"}
## Exercise 8

Next, let's see if we can predict reaction times. As a first step, make a histogram of reaction times, with separate facets for each `Q_type`
:::

```{r}

```

# Exercise 9
::: {.callout-note icon="false"}
## Exercise 9

Reaction times are not typically normally distributed, because they have a minimum at 0, and tend to have a long tail (a small number of trials with very long RTs). We can use a log transformation to try to make them closer to normal distribution, which will allow the data to fit the assumptions of our models.

Make a new variable, `logRT`, and plot the histograms again for that variable.
:::

```{r}

```

# Exercise 10
::: {.callout-note icon="false"}
## Exercise 10

Next, we'll make three plots. In each case, the y-axis will be `logRT`, and the x-axes will be:

1. `Q_type`
2. `chose_targ`
3. `trialnum`

Rather than plotting the raw data, plot the means for each level of the variable on the x-axis. You can do this in a few ways:

- Use `group_by()` and `summarise()` to get the means in a separate dataframe before plotting
- In your plot, use `stat_summary()` instead of `geom_point()` (Tip: with `stat_summary`, set `group = 1` if you get an error about each group having only one observation)
- In your plot, use `geom_smooth()` to plot smoothed conditional means.
:::

```{r}

```

# Exercise 11
::: {.callout-note icon="false"}
## Exercise 11

What kind of model(s) that you've learned so far would be appropriate for predicting reaction time? Pick an appropriate model type and follow the same basic steps you did before:

1. Make the train/test split (we need to redo this, since we didn't have `logRT` in our dataset before)
2. Make the model specification and workflow
3. Build one model predicting `logRT` from just `Q_type`
4. Build a second model predicting `logRT` from `Q_type`, `chose_targ` and `trialnum`
5. Pick appropriate metrics and evaluate your models on the training data. Which do you think will be best?
6. Evaluate your models on the test data. Was your guess correct?
:::

```{r}

```

# Exercise 12
::: {.callout-note icon="false"}
## Exercise 12

Make one last model with all predictors from the previous model, but also include `subj_id` as a predictor. 

- Does this model perform better or worse on the **training** set?
- Does it perform better or worse on the **test** set? Why?
:::

```{r}

```



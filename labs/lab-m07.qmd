---
title: "Module 7 Lab: NYC taxi trips"
author: "Derek Powell"
bibliography: ../references.bib
editor: source
---


```{r}
#| include: false
library(tidyverse)
library(tidymodels)
library(hutils)

taxi <- read_csv("data/nyc-taxi.csv") %>% 
  mutate(pickup_datetime = lubridate::as_datetime(pickup_datetime))

```

![](../imgs/nyc-taxi.jpg)

::: {.callout-caution icon="false"}
## Learning Objectives

Our learning objectives for this module are:

-   Apply non-linear modeling techniques with decision trees, random forests, and boosted trees
-   Gain experience using recipes to conduct feature engineering and visualization
-   Become acquainted with using `workflow_set()` and `workflow_map()` functions of `tidymodels` to efficiently compare models
-   Conduct an efficient hyperparameter tuning search and compare against untuned models.
:::

In this lab assignment we will be building a predictive model to predict NYC taxi travel times. 

Taxi cabs are a quintessential part of the everyday rhythm of New York City life. They offer a reliable and convenient means of transportation for residents and visitors alike: From commuters navigating the city's diverse neighborhoods to tourists exploring its iconic landmarks, taxis serve as dependable allies in navigating the urban landscape. Whether hailed on the street or summoned via a mobile app, these yellow cabs are an integral part of NYC's charm, providing a familiar sight amid the hustle and bustle of city life.

At the same time, predicting travel times, delivery schedules, and other logistical factors is crucial for individuals, businesses, and governments alike. For individuals, accurate predictions enable efficient planning of daily activities, reducing stress and enhancing productivity. Businesses rely on such forecasts to optimize supply chain management, minimize costs, and maintain customer satisfaction through timely deliveries. Additionally, governments utilize predictive models to improve traffic management, urban planning, and public transportation systems, ultimately enhancing overall city functionality and citizen well-being. In essence, accurate predictions of travel and delivery times are fundamental for enhancing efficiency, reducing costs, and improving the quality of life for everyone involved.

# Preliminaries

## The data

We will be using data describing `r nrow(taxi)` NYC taxi rides in 2016. This is a small subset of a larger dataset of 1.5 million taxi rides that was the subject of a [Kaggle competition](https://www.kaggle.com/c/nyc-taxi-trip-duration).

Use the code below to set up your environment and load the data.

```{r}
#| message: false
library(tidyverse)
library(tidymodels)
library(hutils)

taxi <- read_csv("data/nyc-taxi.csv") %>% 
  mutate(pickup_datetime = lubridate::as_datetime(pickup_datetime))
```


## Setting up parallel processing

We will be performing some computationally intensive training and tuning steps that benefit from parallelization: spreading the training of individual models across multiple CPU cores.

We can configure this with the `doMC` package, setting cores to 4 to match how I've set up your posit cloud instances. Relevant `tidymodels` functions will then automatically use parallelization as defined here by default. 

Copy and paste the code below and run it to set up parallelization in your environment.

```{r}
#| message: false
library(doMC)
registerDoMC(cores = 4)
```


# Exploratory Data Analysis

We'll begin as we generally should with a quick exploratory data analysis and some visualizations of our data.

## Exercise 1
::: {.callout-note icon="false"}
## Exercise 1

Set up the data in train and test splits (call the datasets `train` and `test`), with a 10-fold cross validation (call this `folds`).
:::


## Exercise 2
::: {.callout-note icon="false"}
## Exercise 2

Let's visualize the training data by plotting the pickup latitude and longitude points. Because there are so many points, we'll want to set `size` to be `.1` and `alpha` to `.2`.
:::

## Exercise 3
::: {.callout-note icon="false"}
## Exercise 3
Let's also plot the distribution of our outcome variable, `trip_duration`. These values are log-transformed. Plot them as they are in the dataset, and also after exponentiating them back onto their original scale. What do you think the original values represent? (seconds? minutes ? hours?)
:::


# Fitting models

Now we're ready to begin fitting some models. Let's quickly compare a few types of models using a workflow set.

## Exercise 4
::: {.callout-note icon="false"}
## Exercise 4
Let's fit some models!

- Create a recipe called `base_rec` that predicts `trip_duration` from all variables. Your recipe should convert all variables into numeric variables (dummy coding nominal variables) so that the data can be used for the boosted tree model (`xgboost` requires all predictors be numeric).
- Create a workflow set that:
  - Includes the recipe as the preprocessor with `preprocessor = list(base = base_rec)`
  - And includes as models a decision tree, random forest, boosted tree, and linear regression model (remember, they have to be put together in a list)
- Using `workflow_map`, fit these models to the cross-validation folds. 
- Examine their performance using `collect_metrics`
:::


<!-- ## Exercise 5 -->
<!-- ::: {.callout-note icon="false"} -->
<!-- ## Exercise 5 -->

<!-- Let's add an additional metric, mean absolute error. Create a metric set called `custom_metrics` that calculates $R^2$, $RMSE$ and $MAE$. Re-run the model training and evaluation with this passed into your `workflow_map()` with the `metrics` argument. -->

<!-- By how many seconds will our estimates typically be off by? Exponentiate the $MAE$ values to find this answer. -->
<!-- ::: -->


# Feature engineering

We might be able to improve our predictions by engineering some new features to help our models uncover important relationships. For instance, **distance** might be an important variable for predicting the duration of a trip. Distance information is already in the data we have, in the latitude and longitude values for pickup and dropoff. However, we might make that information easier for our model to use by directly computing a variable to represent it.

## Exercise number 5
::: {.callout-note icon="false"}
## Exercise number 5

Create a new recipe called `fe_rec` that will include some additional feature engineering steps.

- Create a new variable called `dist_km` using the `haversine_distance()` function from the `hutils` package. Look at the help if you need help.
- Create a new variable called `bearing` using the `bearing()` function from the `hutils` package. This will indicate the overall direction of travel for the trip.
- Use the `step_date()` and `step_time()` recipe functions to create features for the day of the week, day of year, and decimal representation for the time of day. You can remove the original datetime variable with the `keep_original_cols` argument or in its owns step.

Then, prep and juice your recipe and store it as `train_juiced`.
:::


## Exercise 6
::: {.callout-note icon="false"}
## Exercise 6

Visualize some of your new features. 

1. Examine the relationship between `trip_duration` and distance. Use a small point size and low alpha so you can see individual points better.
2. Examine how the time of day affects the average speed of travel across the city. Outside of your recipe, mutate the `train_juiced` data to round your decimal time of day variable to the 1 decimal place and compute a new variable `speed_kmph`. Plot the average speed across each of the new time of day values. For bonus points, add a `geom_ribbon()` with a 95% confidence interval (i.e. $1.96 \times$ the standard error).

:::



## Exercise 7
::: {.callout-note icon="false"}
## Exercise 7
Briefly answer two questions:

1. What do you make of the second plot? Can you explain what is going on?
2. Do you think these new features might help your models? If so, which will each help?
:::

## Exercise 8
::: {.callout-note icon="false"}
## Exercise 8

Using `workflow_map()` fit the same models with the new feature engineering recipe. Then, collect and display the results. What do you find?
:::

# Hyperparameter tuning

Let's make our models even better! Unlike a basic linear regression model, models like boosted trees need some tuning of their hyperparameters to perform their best.

::: callout-tip
For further details and examples that might help with this set of exercises, I'll refer you to Sections 12.2, 12.5-12.6, 13.1, 13.5.5, and Chapter 15 of [Tidy Modeling with R](https://www.tmwr.org/tuning#tuning-parameter-examples).
:::

## Exercise 9
::: {.callout-note icon="false"}
## Exercise 9

- Make new random forest and boosted tree models and pass in the `tune()` function for their important parameters: `mtry` for the random forest model, and `trees`, `learn_rate` for the boosted tree model.
- Set a seed as there is some randomization involved in the next step
- Run a new `workflow_map()` to compare your linear regression, random forest, and boosted tree models using the feature engineering preprocessor. Note that running this step may take 20+ minutes. You will need to set a few special arguments:
    - First, set the arguments `fn = "tune_race_anova"` and `grid = 10`. This will create a grid with (up to) 10 different parameter combinations and conduct a racing parameter search to find the best hyperparameters for each model.
    - Also, set the argument `control = control_race(save_workflow = TRUE)` to save the workflows so they can be extracted later.
- Rather than just collecting the metrics, plot them as a pointrange plot (with the errorbars representing standard errors). Make sure to sort from best to worst, and color things by the model type.

What do you make of the performance and the effect of our extra tuning effort?
:::

::: {.callout-caution}
## Warning

If this is just taking too long to run on Posit cloud, you can run this exercise using just a sample of rows of your training dataset (e.g. 5k or 10k rows).
:::


## Exercise 10

It's time to re-train our best model on our full training data set and evaluate its performance on the test data. 

::: {.callout-note icon="false"}
## Exercise 10

Extract the best-performing workflow from your workflow set and re-train it on the full training data. 

Then, calculate its predictive performance on the test data. 

:::

::: callout-tip
To extract the best-fitting workflow, you'll need to take a few steps:

1. First, you'll need to identify which type of model performed best. You can do this by plotting the results, which you already did above. (A shortcut to do this is `autoplot()`).
2. Then, you can use `extract_workflow_set_result()` to extract the results corresponding to the correct workflow id from the output of your `workflow_map()` step.
3. You can use `select_best()` on those workflow set results to get your best-performing parameters.
4. And you can then either manually set the parameters to match those of your best-fitting workflow, or use `finalize_workflow()` to set them from the result.

Roughly, this could look something like:

```r
best_mod <- wf_map_result %>% 
  extract_workflow_set_result("best_wf_name") %>% 
  select_best("rmse") # or other metric name

final_wf <- workflow() %>% 
  add_recipe(my_recipe) %>% 
  add_model(my_tunable_model) %>% 
  finalize_workflow(best_mod)
  
```
:::



# Wrapping up

When you are finished, knit your Quarto document to a PDF file.

::: {.callout-important icon="false"}
**MAKE SURE YOU LOOK OVER THIS FILE CAREFULLY BEFORE SUBMITTING**
:::

When you are sure it looks good, submit it on Canvas in the appropriate assignment page.
---
title: "Machine Learning: The Workflow"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
bibliography: ../references.bib
---
```{r}
#| include: false
library(tidyverse)
```

## Machine learning: The basic idea 

::: {.center}
```{mermaid}
%%| fig-width: 9.5
%%| mermaid-format: svg
flowchart LR
  A[(Data)] -- used to make --> B[Model] -- used to make --> C{{Predictions}}
```
:::

## Test data

Beginning at the end:

::: {.incremental}
- We want to make prediction for ***new*** data. 
- To know whether our predictions will be good, need to test our models on ***new*** data
  - Not the same data that was used to make the model!
:::

## Splitting the ~~baby~~ data

:::: {.columns}

::: {.column width="50%"}

We split our data into training and testing splits:

:::

::: {.column width="50%"}

```{r}
#| echo: false
#| fig-height: 3

tibble(
  Split = ordered(c("Train", "Test"), levels = c("Train", "Test")),
  prop = c(.80, .20)
) %>% 
  ggplot(aes(x=-prop, y = "", fill = Split)) +
  geom_bar(stat="identity") +
  scale_fill_discrete() +
  theme_minimal(base_size = 18) +
  theme(
    legend.position = "top",
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    aspect.ratio = 1/3
    )
  
```
:::

::::

Using some for **Training** and some for **Testing**:

```{mermaid}
%%| mermaid-format: svg
%%{init: {"flowchart": {"htmlLabels": false}} }%%
graph LR
  d[(Data)] --> split([Split]) --> trainSplit[(Training)]
  split --> testSplit[(Testing)]
  trainSplit --> model[Model]
  model --> val{{Final Validation}}
  testSplit  ---> val
  
```

## Machine learning: Making good model(s)


:::: {.columns}

::: {.column width="50%"}

- Building the best model often means making many models and picking the best one
- Remember: ***Best*** meaning best at predicting for ***new*** data

::: {.fragment}
Whatever data we use to pick the model won't be new data!
:::

:::

::: {.column width="50%"}


```{mermaid}
%%| mermaid-format: svg
%%| fig-width: 4.5
graph LR
  trainSplit[(Training)] --> m1[Model 1] & m2[Model 2] & m3[...] & m4[Model j]
  
  m1 & m2 & m3 & m4 --> sel{{Model selection}}

```

:::

::::



## Validation data

One solution: create another ***validation*** split of data.

```{r}
#| echo: false
#| fig-height: 3

tibble(
  Split = ordered(c("Train", "Test", "Validation"), levels = c("Train", "Validation", "Test")),
  prop = c(.6, .2, .2)
) %>% 
  ggplot(aes(x=-prop, y = "", fill = Split)) +
  geom_bar(stat="identity") +
  scale_fill_discrete() +
  theme_minimal(base_size = 18) +
  theme(
    legend.position = "top",
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank(),
    aspect.ratio = 1/3
    )
  
```
- ***Train*** models on the [training data]{.text-red}
- ***Compare*** and select best model using the [validation data]{.text-green}
- ***Predict*** future model performance using the [test data]{.text-blue}

## Data Budget

::: {.callout-note icon="false"}
## Think about it ...
Aren't we going to start running out of data?
:::

::: {.incremental}
- More training data $\rightarrow$ better models
- More validation/testing data $\rightarrow$ more accurate validation
::: 

::: {.fragment}
How can we use as much data as possible to get the best models and evaluations?
::: 

## Cross Validation

```{r}
library(patchwork)

col_ <- function(x, nrow, ncol){
  col <- floor((x-1)/ncol) + 1
  return(col)
}

row_ <- function(x, nrow, ncol){
  return( -1*((x-1) - nrow*col_(x, nrow, ncol) + 1 ) )
}

ids <- seq(1,25,1)
shuffled_ids <- dplyr::row_number(runif(25))

## make 5 copies
expand_grid(
  id = ids,
  fold = c(1,2,3,4,5)
) %>% 
  group_by(fold) %>% 
  mutate(
    order_id = id,
    row = -row_(order_id, 5, 5),
    col = -col_(order_id, 5, 5),
    # Split = if_else(-col==fold, "Validation", "Train"),
    fold = paste("Fold", fold)
    ) %>% 
  ggplot(aes(x = row, y = col, label = id)) +
  geom_tile(fill = "lightgrey", color = "black") +
  geom_text(size=3) +
  facet_wrap(~fold, nrow = 1) +
  # scale_fill_manual(values = c("lightyellow", "lightblue")) +
  theme_minimal() +
  theme(
    aspect.ratio=1,
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank()
    ) +
  labs(title = "Make k copies of the data") +

## Randomize ...
expand_grid(
  id = ids,
  fold = c(1,2,3,4,5)
) %>% 
  group_by(fold) %>% 
  mutate(
    order_id = shuffled_ids,
    row = -row_(order_id, 5, 5),
    col = -col_(order_id, 5, 5),
    # Split = if_else(-col==fold, "Validation", "Train"),
    fold = paste("Fold", fold)
    ) %>% 
  ggplot(aes(x = row, y = col, label = id)) +
  geom_tile(fill = "lightgrey", color = "black") +
  geom_text(size=3) +
  facet_wrap(~fold, nrow = 1) +
  # scale_fill_manual(values = c("lightyellow", "lightblue")) +
  theme_minimal() +
  theme(
    aspect.ratio=1,
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank()
    ) +
  labs(title = "Randomize ...") +

## ... and assign to train and test splits
expand_grid(
  id = ids,
  fold = c(1,2,3,4,5)
) %>% 
  group_by(fold) %>% 
  mutate(
    order_id = shuffled_ids,
    row = -row_(order_id, 5, 5),
    col = -col_(order_id, 5, 5),
    Split = if_else(-col==fold, "Validation", "Train"),
    fold = paste("Fold", fold)
    ) %>% 
  ggplot(aes(x = row, y = col, label = id)) +
  geom_tile(aes(fill=Split), color = "black") +
  geom_text(size=3) +
  facet_wrap(~fold, nrow = 1) +
  scale_fill_manual(values = c("lightyellow", "lightblue")) +
  theme_minimal() +
  theme(
    aspect.ratio=1,
    panel.grid = element_blank(),
    axis.title = element_blank(),
    axis.text = element_blank()
    ) +
  labs(title = "... and assign different train/test splits within each fold") +
  plot_layout(ncol = 1)
  
```


## Cross Validation workflow

```{mermaid}
%%| mermaid-format: svg

graph LR
  trainSplit[(Training)] --> resamp([Resampling])
  resamp --> m1 & m2 & m3 & m4
  
  subgraph K-folds
  m1[Model 1] & m2[Model 2] & m3[...] & m4[Model j]
  end

  m1 & m2 & m3 & m4 --> sel{{Model selection}}

```

## The overall machine learning workflow

```{mermaid}
%%| fig-width: 9.5
%%| mermaid-format: svg
graph LR
  d[(Data)] --> split([Split]) --> trainSplit[(Training)]
  split --> testSplit[(Testing)]
  trainSplit --> resamp([Resampling])
  resamp --> m1 & m2 & m3 & m4
  
  subgraph K-folds
  m1[Model 1] & m2[Model 2] & m3[...] & m4[Model j]
  end

  m1 & m2 & m3 & m4 --> sel{{Model selection}}
  sel --> final[Final Model]
  trainSplit --> final
  final --> val{{Final Validation}}
  testSplit ------> val
  
  
```

## Before the workflow

- Data acquisition
- Data preprocessing + cleaning
- Domain understanding
- Exploratory data analysis + visualization

::: {.fragment}
[***AKA: Data Science!***]{.text-red}
:::

## After the workflow

:::: {.columns}

::: {.column width="50%"}

### Practical Implementation

- Efficiency
- Performance Monitoring
- Model updating and refinement

:::

::: {.column width="50%"}

### Insight and inference

- Feature importance
- Visualization and insights

:::

::::


# Implementing the workflow in `tidymodels`

## A case study: Bikesharing

> Capital Bikeshare is metro DC's bikeshare service, with 4,500 bikes and 500+ stations across 7 jurisdictions: Washington, DC.; Arlington, VA; Alexandria, VA; Montgomery, MD; Prince George's County, MD; Fairfax County, VA; and the City of Falls Church, VA. Designed for quick trips with convenience in mind, itâ€™s a fun and affordable way to get around.

We will consider a dataset that includes the total number of Capital Bikeshare rides from 500 separate days.

## Splitting data into training and testing splits

```{r}
#| message: false
#| echo: true
library(tidyverse)
library(tidymodels)

bikes <- read_csv("data/bikes.csv")

set.seed(2356)
splits <- initial_split(bikes, prop = .8, strata = month)

train <- training(splits)
test <- testing(splits)
```

## Setting up Cross Validation

```{r}
#| message: false
#| echo: true
folds <- vfold_cv(train, v = 5)
folds
```

## Creating models

```{r}
#| message: false
#| echo: true
lin_model <- linear_reg()

m1 <- workflow(rides ~ temp_feel) %>% 
  add_model(lin_model)

m2 <- workflow(rides ~ temp_feel + humidity + windspeed + day_of_week + holiday + weather_cat) %>% 
  add_model(lin_model)
```


## Fitting models and selecting the best

```{r}
#| message: false
#| echo: true
fit1 <- fit_resamples(m1, folds)
fit2 <- fit_resamples(m2, folds)

collect_metrics(fit1)
collect_metrics(fit2)
```


## Validating on the test data

```{r}
#| message: false
#| echo: true
preds <- fit(m2, train) %>% 
  augment(test)

preds %>% 
  metrics(rides, .pred)
```
## Visualizing the results

```{r}
#| message: false
#| echo: true
preds %>% 
  ggplot(aes(x=.pred, y = rides)) +
  geom_point() +
  theme(aspect.ratio=1)
```


---

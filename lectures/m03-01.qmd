---
title: "Introduction to Logistic Regression"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
execute:
  echo: true
bibliography: ../references.bib
---

```{r}
#| include: false

library(tidyverse)
theme_set(theme_grey(base_size = 20))
```

## Classification

Classification involves predicting **discrete** categorical outcomes.

- Whether an event/outcome occurred
- To which class does an observation belong
- Etc.

## Regression

**Regression** techniques aim to predict **continuous** quantities (e.g. height, income, test performance, response time, etc.)

## Classification with Logistic Regression

We can use **regression** techniques to predict the **probability** of discrete outcomes.

## Example: Binary election outcomes

- Recall our example of the `hibbs` data connecting economic growth and U.S. Presidential election outcomes
- We can compute a binary categorical outcome from our `hibbs` data to indicate whether the incumbent won (`1`) or lost (`0`)

```{r}
library(tidyverse)
library(tidymodels)
library(rosdata)

data(hibbs)

df <- hibbs %>% 
  mutate(inc_won = if_else(vote > 50, 1, 0))

```

## Classification with Linear Regression?

::: {.callout-note icon="false"}
## Think about it ...
What if we tried to use linear regression to predict the probability of binary outcomes? 
:::

Let's try it!

## Linear regression on binary outcomes

```{r}
linfit <- lm(inc_won ~ growth, df)

pred_df <- df %>% 
  mutate(
    .pred = predict(linfit, .),
    .pred1 = if_else(.pred > .50, 1, 0),
    )
  
pred_df %>% 
  summarize(acc = mean(.pred1 == inc_won))

```
## What' wrong with this picture?

```{r}
#| fig-align: center
#| echo: false
tibble(
  growth = c(min(df$growth)-.75, max(df$growth)+.75)
  ) %>% 
  mutate(.pred = predict(linfit, .)) %>% 
  ggplot(aes(x = growth, y = .pred)) +
  geom_line() +
  geom_point(data = df, aes(y = inc_won)) +
  scale_y_continuous(breaks = c(0, .25, .5, .75, 1))
```

::: {.fragment}
Probabilities must fall between $[0, 1]$, but our model predicts probabilities $p < 0$ and $p > 1$!
:::



## A solution: A link function

- We need some way to *smoosh* our models' predicted probabilities so that they always fall between zero and one.
- The **Logistic** link function does just that.


```{r}
#| output-location: column

tibble(
  x = seq(-5, 5, .01),
  x_logistic = plogis(x)
) %>% 
  ggplot() +
  aes(x= x, y = x_logistic) +
  geom_line()
```



## Generalized linear models

- **Logistic Regression** is an example of a **Generalized Linear Model**, a family of modeling approaches that extend the basics of linear regression to apply to a range of scenarios.
- These models add a **link function** $f$ to the basic linear model:

$$\hat{y} =  f(\alpha + \beta x)$$
## Logistic Regression

- **Logistic Regression** is a **Generalized Linear Model** 
- Incorporating a **logistic** link function.

The probability of an event $p(y)$ is estimated with a linear equation, smooshed between zero and one with the logistic link.

$$\hat{p(y)} =  \text{logistic}(\alpha + \beta x)$$
## `hibbs` logistic model predictions

```{r}
#| echo: false
lrfit <- glm(inc_won ~ growth, family = "binomial", data = df)

tibble(
  growth = modelr::seq_range(df$growth, 100)
  ) %>% 
  mutate(.pred = predict(lrfit, ., type = "response")) %>% 
  ggplot(aes(x = growth, y = .pred)) +
  geom_line() +
  geom_point(data = df, aes(y = inc_won))
```





---
title: "Introduction to Logistic Regression"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
execute:
  echo: true
bibliography: ../references.bib
---

```{r}
#| include: false

library(tidyverse)
theme_set(theme_grey(base_size = 20))
```

## Classification

Classification involves predicting **discrete** categorical outcomes.

- Whether an event/outcome occurred
- To which class does an observation belong
- Etc.

## Regression

**Regression** techniques aim to predict **continuous** quantities (e.g. height, income, test performance, response time, etc.)

## Classification with Logistic Regression

We can use **regression** techniques to predict the **probability** of discrete outcomes.

## Example: Binary election outcomes

- Recall our example of the `hibbs` data connecting economic growth and U.S. Presidential election outcomes
- We can compute a binary categorical outcome from our `hibbs` data to indicate whether the incumbent won (`1`) or lost (`0`)

```{r}
library(tidyverse)
library(tidymodels)
library(rosdata)

data(hibbs)

df <- hibbs %>% 
  mutate(inc_won = if_else(vote > 50, 1, 0))

```

## Classification with Linear Regression?

::: {.callout-note icon="false"}
## Think about it ...
What if we tried to use linear regression to predict the probability of binary outcomes? 
:::

Let's try it!

## Linear regression on binary outcomes

```{r}
linfit <- lm(inc_won ~ growth, df)

pred_df <- df %>% 
  mutate(
    .pred = predict(linfit, .),
    .pred1 = if_else(.pred > .50, 1, 0),
    )
  
pred_df %>% 
  summarize(acc = mean(.pred1 == inc_won))

```
## What' wrong with this picture?

```{r}
#| fig-align: center
#| echo: false
tibble(
  growth = c(min(df$growth)-.75, max(df$growth)+.75)
  ) %>% 
  mutate(.pred = predict(linfit, .)) %>% 
  ggplot(aes(x = growth, y = .pred)) +
  geom_line() +
  geom_point(data = df, aes(y = inc_won)) +
  scale_y_continuous(breaks = c(0, .25, .5, .75, 1))
```

::: {.fragment}
Probabilities must fall between $[0, 1]$, but our model predicts probabilities $p < 0$ and $p > 1$!
:::



## A solution: A link function

- We need some way to *smoosh* our models' predicted probabilities so that they always fall between zero and one.
- The **Logistic** link function does just that.


```{r}
#| output-location: column

tibble(
  x = seq(-5, 5, .01),
  x_logistic = plogis(x)
) %>% 
  ggplot() +
  aes(x= x, y = x_logistic) +
  geom_line()
```



## Generalized linear models

- **Logistic Regression** is an example of a **Generalized Linear Model**, a family of modeling approaches that extend the basics of linear regression to apply to a range of scenarios.
- These models add a **link function** $f$ to the basic linear model:

$$\hat{y} =  f(\alpha + \beta x)$$
## Logistic Regression

- **Logistic Regression** is a **Generalized Linear Model** 
- Incorporating a **logistic** link function.

The probability of an event $p(y)$ is estimated with a linear equation, smooshed between zero and one with the logistic link.

$$\hat{p(y)} =  \text{logistic}(\alpha + \beta x)$$
## `hibbs` logistic model predictions

```{r}
#| echo: false
lrfit <- glm(inc_won ~ growth, family = "binomial", data = df)

tibble(
  growth = modelr::seq_range(df$growth, 100)
  ) %>% 
  mutate(.pred = predict(lrfit, ., type = "response")) %>% 
  ggplot(aes(x = growth, y = .pred)) +
  geom_line() +
  geom_point(data = df, aes(y = inc_won))
```


## Loss Function & Likelihood

For logistic regression, the loss function is based on **likelihood** of the data. 

Where $y_i \in \{0, 1\}$:

$$\mathcal{L(y_i, p_i)} = \prod_i p_i^{y_i} \prod_i (1-p_i)^{1 - y_i}$$

::: {.callout-tip icon="false"}
## In words:
"The product of the model's predicted probability of positive for all the positive cases times the product of the models predicted probability of negative for all negative cases."
:::

## Cross-Entropy Loss [^1]

- In log-space, multiplication becomes addition and exponentiation becomes multiplication.
- So, it is generally easier to define the loss to be minimized ($\mathcal{l}$) in terms of the negative log-likelihood:

$$ \mathcal{l(y_i, p_i)} = - \sum_i y_i log(p_i) + (1-y_i) log(1-p_i)$$

[^1]: AKA: the binomial log likelihood loss

## Likelihood: Hypothetical example

:::: {.columns}
::: {.column width="50%"}
::: {style="font-size: 80%;"}
Consider the following observations for `y` and predicted model probabilities from two models, `p1` and `p2`.
:::


:::
::: {.column width="50%"}

```{r}
#| echo: false
res <- tibble(
  y = c(1,1,0,0),
  p1 = c(.56, .61, .78, .45),
  p2 = c(.48, .93, .06, .35)
)
res
```

:::
::::

The point-wise likelihood of the data is as follows:

```{r}
res %>% 
  mutate(
    likeli_pw1 = p1^y * (1-p1)^(1-y),
    likeli_pw2 = p2^y * (1-p2)^(1-y)
  )
```

## Likelihood: Hypothetical example

The likelihood of the data is the product of those point-wise likelihoods:

```{r}
res %>% 
  mutate(
    likeli_pw1 = p1^y * (1-p1)^(1-y),
    likeli_pw2 = p2^y * (1-p2)^(1-y)
  ) %>% 
  summarize(
    likeli1 = prod(likeli_pw1),
    likeli2 = prod(likeli_pw2)
  )
```


## Cross-entropy loss: Hypothetical example {auto-animate="true"}

For the cross-entropy loss, these same calculations take place in log-space:

```{r}
res %>% 
  mutate(
    likeli_pw1 = y*log(p1) + (1-y)*log(1 - p1),
    likeli_pw2 = y*log(p2) + (1-y)*log(1- p2)
  ) %>% 
  summarize(
    loss1 = -sum(likeli_pw1),
    loss2 = -sum(likeli_pw2),
  )
```

## Cross-entropy loss: Hypothetical example {auto-animate="true"}

For the cross-entropy loss, these same calculations take place in log-space:

```{r}
res %>% 
  mutate(
    likeli_pw1 = y*log(p1) + (1-y)*log(1 - p1),
    likeli_pw2 = y*log(p2) + (1-y)*log(1- p2)
  ) %>% 
  summarize(
    loss1 = -sum(likeli_pw1),
    loss2 = -sum(likeli_pw2),
  ) %>% 
  mutate_all(~exp(-.x))
```

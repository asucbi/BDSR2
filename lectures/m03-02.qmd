---
title: "The Logistic Function"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
bibliography: ../references.bib
---

## Logistic Function

The **Logistic** function smooshes continuous values in the range $[-\infty, \infty]$ to fall between zero and 1.

```{r}
#| echo: false
#| fig-align: center
library(tidyverse)
theme_set(theme_minimal(base_size = 20))

tibble(
  x = seq(-5, 5, .01),
  x_logistic = plogis(x)
) %>% 
  ggplot() +
  aes(x= x, y = x_logistic) +
  geom_line() +
  labs(y = "logistic(x)")
```


## Working backwards

- To understand how it works, we'll work backwards to consider how we might expand probabilities from zero to one onto the range $[-\infty, \infty]$
- Then we'll see how the logistic function does just the opposite of that!

## Odds

:::: {.columns}
::: {.column width="50%"}

::: {style="font-size: 65%;"}

If you are a betting man (or woman), you know that **odds** are another way to express uncertainty:

$$o(x) = \frac{p(x)}{1-p(x)}$$

::: {.fragment}
Commonly used to express a fair bet: 

- If we agree an event has a 75% probability, then to bet against it you should need to gain \$3 for every \$1 risked.  
- So, instead of saying "75% probability" we might also say "3:1 odds".
:::


:::

:::
::: {.column width="50%"}
![](../imgs/DALLE-2024-01-02-09.26.43-bookie board.png)
:::
::::



## Functions and inverse functions

::: {.callout-caution icon="false"}
## Notation
$f^{-1}$: the inverse of the function $f$.  The notation extends the notion of the inverse of a number expressed in negative exponents, $x^{-1} = 1/x$.
:::

If we define $g$ to be a function that transforms probabilities into odds, then its **inverse function** $g^{-1}$ is a function that transforms odds into probabilities.

:::: {.columns}
::: {.column width="50%"}
$$g(p) = \frac{p}{1-p}$$
:::
::: {.column width="50%"}
$$g^{-1}(o) = \frac{o}{o+1}$$
:::
::::


## Functions and outputs

Very loosely ...

- Probabilities fall $[0, 1]$
- Odds fall in  $[0, \infty]$
- Linear equations are unconstrained, operating in $[-\infty, \infty]$

## Logarithms and Exponentiation

**Exponentiation**: $b^n = \underbrace{b \times b \times ... \times b}_{n \text{ times}}$

**Logarithm**: The inverse of exponentiation

$$y = log_bx \iff b^y=x$$
::: {.callout-caution icon="false"}
## Notation
$\text{ln} \ x$: The natural logarithm of $x$, using euler's number $e \approx 2.718$ as the base. I.e., equivalent to $log_e x$.
:::


## Logarithms and Exponentiation in `R`

`log()` computes the natural logarithm and `exp()` the exponentiation function (default natural base).

```{r}
#| echo: true

log(2)
exp(2)
exp(log(2))
log(exp(2))
```


## Log-odds and the logit

- $\text{ln} \ x$ is defined for all positive numbers and the resulting values fall in $[-\infty, \infty]$
- Odds are a method for expressing uncertainty in positive numbers
- So log-odds are a method for expressing uncertainty with real numbers

::: {.fragment}
The **logit** function:

$$\text{logit}(p) =  \text{ln} \ \frac{p}{1-p}$$
:::


## The inverse-logit: The logistic function!

- By transforming probabilities into log odds, we expand values from the range $[0, 1]$ to the range $[-\infty, \infty]$.
- By doing just the opposite (inverse), we can smoosh our linear model predictions into the range of probabilities! And that's the **logistic** function!

$$\text{logistic}(x) = \text{logit}^{-1}(x) = \frac{1}{1 + e^{x}}$$
## Logistic Regression

Substituting in our linear equation, we have logistic regression:

$$\hat{p(y)} = \frac{1}{1 + e^{\alpha + \beta x}}$$

## Another link idea: Clamping

What if we instead just "clamp" the values of a linear probability model so that they are forced to fall in $[0,1]$?

```{r}
#| fig-align: center
#| echo: false

library(tidyverse)
library(tidymodels)
library(rosdata)

data(hibbs)

df <- hibbs %>% 
  mutate(inc_won = if_else(vote > 50, 1, 0))

lrfit <- glm(inc_won ~ growth, family = "binomial", data = df)
linfit <- lm(inc_won ~ growth, df)

tibble(
  growth = seq(min(df$growth)-.75, max(df$growth)+.75, .01)
  ) %>% 
  mutate(.pred = predict(linfit, .)) %>% 
  mutate(.pred = case_when(
    .pred < 0 ~ 0,
    .pred > 1 ~ 1,
    TRUE ~ .pred
  )
  ) %>% 
  ggplot(aes(x = growth, y = .pred)) +
  geom_line() +
  geom_point(data = df, aes(y = inc_won)) +
  scale_y_continuous(breaks = c(0, .25, .5, .75, 1))
```

## Another problem: Which differences matter?

- Is the difference between 51% and 63% probability the same as:
  - the difference between 87% and 99% probability?
  - the difference between 1% and 13% probability?

## Linear probability problems

For each 0.25% increase in growth, the linear probability models predicts approx. 5% increase in probability.

:::: {.columns}
::: {.column width="70%"}


```{r}
#| echo: false
tibble(
  growth = seq(min(df$growth)-.75, max(df$growth)+.75, .01)
  ) %>% 
  mutate(.pred = predict(linfit, .)) %>% 
  mutate(.pred = case_when(
    .pred < 0 ~ .001,
    .pred > 1 ~ .999,
    TRUE ~ .pred
  )
  ) %>% 
  ggplot(aes(x = growth, y = .pred)) +
  geom_line() +
  geom_point(data = df, aes(y = inc_won)) +
  geom_point(data = tibble(growth = 3.9, .pred = 1.), size = 30, shape  = 1, color = "red") +
  geom_point(data = tibble(growth = 1.625, .pred = .5), size = 30, shape  = 1, color = "red") +
  geom_point(data = tibble(growth = -0.5, .pred = 0.), size = 30, shape  = 1, color = "red") +
  scale_y_continuous(limits =  c(-.1, 1.1), breaks = c(0, .25, .5, .75, 1))
```
:::
::: {.column width="30%"}

```{r}
tibble(
  growth = c(-.9, -.65, 1.5, 1.75, 3.8, 4.05)
) %>% 
  mutate(.pred = predict(linfit, .))
  # arrange(desc(.pred))
```
:::
::::


## Linear in probabilities vs. linear in log-odds

- For most real-world contexts, it doesn't make much sense to think of differences in probabilities of a response variable as linearly related to changes in any of the independent predictors.
- In logistic regression, the relationship is non-linear in the probabilities and linear in the **log-odds**

## Interpreting logistic regression coefficients

```{r}
tidy(lrfit)
```
- The log-odds of incumbent re-election increase by 1.04 for each percentage point increase in growth
- A linear relationship in log-odds is multiplicative in odds:
  - The odds of incumbent re-election are `exp(1.04)`  = `r round(exp(1.04),2)` **times greater** for each percentage point increase in growth
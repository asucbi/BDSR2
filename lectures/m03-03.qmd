---
title: "Logistic Regression in Practice"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
execute:
  echo: true
bibliography: ../references.bib
---

## Example: Wells in Bangladesh

< introduce this >



## The data

```{r}
#| include: false
library(tidyverse)
library(tidymodels)
library(broom)
```


```{r}
#| echo: true
library(rosdata)

data(wells)

wells <- as_tibble(wells) %>% 
  mutate(switch = factor(if_else(switch == 1, "Yes", "No")))

wells
```

## Exploratory Data Analysis

```{r}
#| echo: true
wells %>% 
  count(switch) %>% 
  mutate(prop = n/sum(n))
```

## EDA: Visualization

::: {.panel-tabset}

## Code

```{r}
#| echo: true
arsenic_dist100_plt <- wells %>%
  pivot_longer(cols = c(arsenic, dist100), names_to = "var", values_to = "val") %>% 
  ggplot(aes(x = switch, y = val)) +
  geom_boxplot() +
  facet_wrap(~var, scales = "free_y")
```

## Plot

```{r}
#| echo: false
arsenic_dist100_plt
```

:::


## EDA: More Visualizations
::: {.panel-tabset}

## Code
```{r}
#| echo: true
arsenic_switching_plt <- wells %>%
  mutate(
    arsenic_bin = cut(percent_rank(arsenic), 20),
  ) %>% 
  group_by(arsenic_bin) %>% 
  mutate(arsenic_bin_x = mean(arsenic)) %>% 
  group_by(arsenic_bin, arsenic_bin_x) %>% 
  summarize(prop = mean(switch=="Yes"), cnt = n()) %>% 
  mutate(se = sqrt(prop*(1-prop)/cnt)) %>%
  ggplot(aes(x = arsenic_bin_x, y = prop, ymin = prop - 2*se, ymax = prop + 2*se)) +
  geom_point() +
  geom_line() +
  geom_ribbon(alpha = .25) +
  labs(x = "Arsenic Bin Avg.",  y = "Prop. switched")
```

## Plot

```{r}
#| echo: false
arsenic_switching_plt
```

:::

## Modeling Setup

```{r}
#| echo: true
set.seed(1234)
splits <- initial_split(wells, prop =.85, strata = switch)

train <- training(splits)
test <- testing(splits)
```


## Simple logistic regression model

```{r}
#| echo: true
lr_wflow1 <- workflow() %>%
  add_formula(switch ~ arsenic) %>% 
  add_model(logistic_reg())

lr_fit1 <- fit(lr_wflow1, data = train)

```
## Evaluating the simple model

```{r}
metrics_unbalanced <- metric_set(accuracy, bal_accuracy)

lr_fit1 %>% 
  augment(test) %>% 
  metrics_unbalanced(truth = switch, estimate = .pred_class)
```
## Interpreting model coefficients

```{r}
#| echo: true

tidy(lr_fit1)
```

- increases log-odds blah  blah

## Plotting model predictions

```{r}
#| echo: false
lr_fit1 %>% 
  augment(
    tibble(arsenic = seq(min(train$arsenic), max(train$arsenic), .1))
  ) %>% 
  ggplot(aes(x = arsenic, y=.pred_Yes)) +
  geom_line(color = "blue") +
  geom_point(data = train, aes( x = arsenic, y = as.numeric(switch=="Yes")), position = position_jitter(height = .075, seed = .42), alpha = .1)
```




## Adding predictors

```{r}
#| echo: true
lr_wflow2 <- workflow() %>%
  add_formula(switch ~ arsenic + dist100) %>% 
  add_model(logistic_reg())

lr_fit2 <- fit(lr_wflow2, data = train)

tidy(lr_fit2)
```

- First look at sign: 
  - As arsenic increases, $p(switching)$ increases
  - As distance increases, $p(switching)$ decreases
- As distance increases, log odds blah blah

## Interpreting predicted probabilities

::: {style="font-size: 80%;"}

```{r}
#| echo: true
ex_data <- crossing(
      arsenic =  quantile(train$arsenic, c(.1, .5, .9)),
      dist100 = quantile(train$dist100, c(.1, .5, .9))
    )

augment(lr_fit2, ex_data)

```
Comparing extremes (90th vs 10th percentile) for $p(switch)$:

- Extreme `arsenic` difference at median `dist100` $\rightarrow .758 - .488 = .270$
- Extreme `dist100` difference at median `arsenic` $\rightarrow .434 - .618 = -.184$

:::

## Standardizing predictors

```{r}
#| echo: true

stdize_rec <- recipe(switch ~ arsenic + dist100, train) %>% 
  step_normalize(all_numeric_predictors())

lr_wflow2 <- workflow() %>%
  add_recipe(stdize_rec) %>% 
  add_model(logistic_reg())

lr_fit2 <- fit(lr_wflow2, data = train)

tidy(lr_fit2)
```

After standardizing, we can see that `arsenic` is actually a more important predictor

## Feature Engineering

```{r}
#| echo: false
x <- train %>%
  mutate(
    arsenic_bin = cut(percent_rank(arsenic), 20),
  ) %>% 
  group_by(arsenic_bin) %>% 
  mutate(arsenic_bin_x = mean(arsenic)) %>% 
  group_by(arsenic_bin, arsenic_bin_x) %>% 
  summarize(prop = mean(switch=="Yes"), cnt = n()) %>% 
  mutate(se = sqrt(prop*(1-prop)/cnt))

lr_fit1 %>% 
  augment(
    tibble(arsenic = seq(min(train$arsenic), max(train$arsenic), .1))
  ) %>% 
  ggplot(aes(x = arsenic, y=.pred_Yes)) +
  geom_line(color = "blue") +
  geom_point(data = train, aes( x = arsenic, y = as.numeric(switch=="Yes")), position = position_jitter(height = .075, seed = .42), alpha = .1) +
  geom_line(data = x, aes(x = arsenic_bin_x, y = prop, ymin = prop - 2*se, ymax = prop + 2*se), linetype = "dashed") +
  geom_ribbon(data = x, aes(x = arsenic_bin_x, y = prop, ymin = prop - 2*se, ymax = prop + 2*se), alpha = .25) +
  xlim(0, 6)
```


## Feature Engineering II

```{r}
#| echo: false
lr_fe <- recipe(switch ~ arsenic, train) %>%
  step_mutate(low_arsenic = if_else(arsenic < .6, "negligible", "noticeable"))
  
lr_wflow1b <- workflow() %>%
  add_recipe(lr_fe) %>%
  add_model(logistic_reg())

lr_fit1b <- fit(lr_wflow1b, data = train)
```

```{r}
#| message: false
#| warning: false 
#| echo: false
x <- train %>%
  mutate(
    arsenic_bin = cut(percent_rank(arsenic), 20),
  ) %>% 
  group_by(arsenic_bin) %>% 
  mutate(arsenic_bin_x = mean(arsenic)) %>% 
  group_by(arsenic_bin, arsenic_bin_x) %>% 
  summarize(prop = mean(switch=="Yes"), cnt = n()) %>% 
  mutate(se = sqrt(prop*(1-prop)/cnt))

lr_fit1b %>% 
  augment(
    tibble(arsenic = seq(min(train$arsenic), max(train$arsenic), .1))
  ) %>% 
  ggplot(aes(x = arsenic, y=.pred_Yes)) +
  geom_line(color = "blue") +
  geom_point(data = train, aes( x = arsenic, y = as.numeric(switch=="Yes")), position = position_jitter(height = .075, seed = .42), alpha = .1) +
  geom_line(data = x, aes(x = arsenic_bin_x, y = prop, ymin = prop - 2*se, ymax = prop + 2*se), linetype = "dashed") +
  geom_ribbon(data = x, aes(x = arsenic_bin_x, y = prop, ymin = prop - 2*se, ymax = prop + 2*se), alpha = .25) +
  xlim(0, 6)
```

## A more complex model

```{r}
#| echo: true

lr_rec <- recipe(switch ~ dist100 + arsenic + educ4, train) %>%
  step_interact(~all_predictors():all_predictors()) %>%
  step_mutate(
    low_arsenic = if_else(arsenic < .6, "negligible", "noticeable"),
    no_educ = if_else(educ4 == 0, 1, 0)
    )
  
lr_wflow3 <- workflow() %>%
  add_recipe(lr_rec) %>%
  add_model(logistic_reg())

lr_fit3 <- fit(lr_wflow3, data = train)
```

## Setting up $k$-fold cross validation

```{r}
set.seed(1234)
folds <- vfold_cv(train, v = 10, strata = switch)
```

## Applying $k$-fold cross validation

::: {.panel-tabset}

## Setup
```{r}
wflows <- as_workflow_set(
  simple = lr_wflow1, 
  two_pred = lr_wflow2, 
  complex = lr_wflow3
  )

cv_res <- workflow_map(
  wflows, 
  fn = "fit_resamples", 
  resamples = folds, 
  metrics = metric_set(accuracy, bal_accuracy)
  )
```

## Results

::: {style="font-size: 80%;"}
```{r}
collect_metrics(cv_res)
```
:::

:::


# Details: Performance metrics

## Accuracy

$$\text{accuracy} = \frac{\text{correct predictions}}{\text{total predictions}}$$

```{r}
#| echo: true
lr_fit3 %>% 
  augment(test) %>% 
  summarize(acc = sum(.pred_class == switch)/n())
```

## Confusion Matrix

```{r}
#| echo: false
tibble(
  actual = factor(c("+", "-", "+", "-"), ordered = TRUE, levels = c("+", "-")),
  predicted = factor(c("+", "-", "-", "+")),
  label = c("True Positive", "True Negative", "False Negative", "False Positive")
) %>% 
  ggplot(aes(x=actual, y = predicted, label = label)) +
  geom_tile(fill = "white", color = "black") +
  geom_text(size = 9) +
  theme_minimal(base_size = 24) +
  theme(
    panel.grid = element_blank(),
    ) +
  labs(x = "Actual", y = "Predicted") +
  scale_x_discrete(position = "top") 
  
```

## True positives and true negatives

::: {style="font-size: 65%;"}

**Manual calculation**

```{r}
lr_fit3 %>% 
  augment(test) %>% 
  summarize(
    acc = sum(.pred_class == switch)/n(),
    TP = sum(.pred_class == "Yes" & switch == "Yes")/sum(switch == "Yes") ,
    TN = sum(.pred_class == "No" & switch == "No")/sum(switch == "No") ,
    )
```

**`tidymodels` calculation**

```{r}
class_metrics <- metric_set(accuracy, bal_accuracy, sensitivity, specificity)

lr_fit3 %>% 
  augment(test) %>% 
  class_metrics(switch, estimate = .pred_class, event_level = "second") 
```
:::


## Accuracy and balanced accuracy

$$\text{balanced accuracy} = \frac{\text{TPR} + \text{TNR}}{2}$$

::: {style="font-size: 50%;"}


**Manual calculation**
```{r}
lr_fit3 %>% 
  augment(test) %>% 
  summarize(
    acc = sum(.pred_class == switch)/n(),
    TP = sum(.pred_class == "Yes" & switch == "Yes")/sum(switch == "Yes") ,
    TN = sum(.pred_class == "No" & switch == "No")/sum(switch == "No") ,
    bal_acc = (TP + TN)/2
    )
```

**`tidymodels` calculation**

```{r}
class_metrics <- metric_set(accuracy, sensitivity, specificity, bal_accuracy)

lr_fit3 %>% 
  augment(test) %>% 
  class_metrics(switch, estimate = .pred_class, event_level = "second") 
```
:::


# tidymodels vs `glm()`

## Comparison with `glm()`
::: {style="font-size: 80%;"}

**`tidymodels`**

```{r}
lr_rec <- recipe(switch ~ dist100 + arsenic + educ4, train) %>%
  step_interact(~ all_predictors():all_predictors()) %>%
  step_mutate(
    low_arsenic = if_else(arsenic < .6, "negligible", "noticeable"),
    no_educ = if_else(educ4 == 0, 1, 0)
  )

lr_wflow3 <- workflow() %>%
  add_recipe(lr_rec) %>%
  add_model(logistic_reg())

fit_tm <- fit(lr_wflow3, data = train)
```

**`glm()`**

```{r}
fit_glm <- glm(
  switch ~ dist100 * arsenic * educ4 + no_educ + low_arsenic,
  family = "binomial",
  data = train %>% mutate(
    no_educ = if_else(educ4 == 0, 1, 0),
    low_arsenic = if_else(arsenic < .6, "negligible", "noticeable")
  )
)
```
:::



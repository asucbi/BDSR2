---
title: | 
  | Techniques for Avoiding Overfitting, Pt. 1
  | Feature Engineering
subtitle: | 
  | Behavioral Data Science in R II
  | Unit 2
  | Module 6
format: 
  revealjs:
    css: style.css
---

## Overview

::: incremental
-   Cross validation
-   **Feature engineering**
    -   Feature selection
    -   Feature extraction
-   **Regularization**
-   Ensemble methods
:::

## The data: cellphone sensors X activity

```{r}
library(knitr)
library(kableExtra)
library(tidyverse)
library(tidymodels)
load('~/Documents/Github/BDSR2/data/HumanActivityRecognition.Rdata') 

df <- df %>% relocate(Activity)

rf_spec <- 
  rand_forest(
    trees=500) %>% 
  set_mode("classification") %>% 
  set_engine(
    "ranger", 
    importance = "impurity")

rec <- recipe(Activity ~ ., train) %>% 
  step_normalize(all_predictors())

rf_fit <- 
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(rf_spec) %>%  
  fit(train) 

# keep = vi(rf_fit)$Variable[1:10]

modify = vi(rf_fit)$Variable[11:112]
for(m in modify){
  col = which(colnames(df)==m)
  df[,col] = rnorm(nrow(df), mean(df[[col]]), sd(df[[col]]))
}

```

```{r}
ggplot(df, aes(x = Activity, fill = Activity))+geom_bar()
```

## The data: cellphone sensors X activity {.nostretch}

```{r, echo=TRUE}
dim(df)
```

::: {style="font-size:50%;"}
```{r}
library(DT)
datatable(df, 
          options = list(
            pageLength = 20, # Number of rows to display
            scrollX = TRUE, # Enable horizontal scrollbar
            scrollY = '400px' # Set vertical scroll and its height
          ),
          width = '100%', # Table width
          height = 'auto' # Table height
)
```
:::

## Potential problems with high-dimensional data

::: incremental
-   Including irrelevant predictors -\> Overfitting
-   Multicollinearity
    -   Less-than-perfect collinearity is fine for prediction, but can be a problem for inference
    -   Perfect collinearity and regression models can't be fit
-   More features -\> more time & data to train
:::

## Feature selection:

### Example Data

```{r}
set.seed(123)
```

```{r}
#| echo: true
sim_lin_df = 
  tibble(
    x1 = rnorm(100,0,1),
    x2 = rnorm(100,0,1),
    x3 = rnorm(100,0,1),
    y = 4.17 + 8.06*x1 + 8.06*x2 + 8.06*x3 + rnorm(100,0,1)
    ) %>% 
  mutate(
    noise1 = rnorm(100,0,1), 
    noise2 = rnorm(100,0,1), 
    perfectCorr_x2 = 2.55 + 3.94*x2, 
    highCorr_x3 = 7.92 + 4.37*x3 + rnorm(100,0,3), 
    linComb_x2_x3 = 17 + 3.1*x2 + 4.7*x3, # x6 is a linear combination of x1 and x2
    zeroVar = rep(1.24, 100), # x5 has zero variance
  ) 
```

```{r}
print("Corr(x3, highCorr_x3):")
cor(sim_lin_df$x3, sim_lin_df$highCorr_x3)
```

## Feature selection:

### Filtering methods

::: r-stack
::: {.fragment .absolute .fade-in-then-out left="0"}
```{r}
#| echo: true 
rec <- recipe(y ~ ., sim_lin_df) %>% 
  step_nzv(all_predictors()) 
```

::: columns
::: {.column width="50%"}
Kept

```{r}
kept = rec %>% 
  prep() %>% 
  juice() %>% colnames()
kept
```
:::

::: {.column width="50%"}
Removed

```{r}
removed = !colnames(sim_lin_df) %in% kept
colnames(sim_lin_df)[removed]
```
:::
:::
:::

::: {.fragment .absolute .fade-in-then-out left="0"}
```{r}
#| echo: true 
rec <- recipe(y ~ ., sim_lin_df) %>% 
  step_nzv(all_predictors()) %>% 
  step_lincomb(all_predictors()) 
```

::: columns
::: {.column width="50%"}
Kept

```{r}
kept = rec %>% 
  prep() %>% 
  juice() %>% colnames()
kept
```
:::

::: {.column width="50%"}
Removed

```{r}
removed = !colnames(sim_lin_df) %in% kept
colnames(sim_lin_df)[removed]
```
:::
:::
:::

::: {.fragment .absolute .fade-in-then-out left="0"}
```{r}
#| echo: true 
rec <- recipe(y ~ ., sim_lin_df) %>% 
  step_nzv(all_predictors()) %>% 
  step_lincomb(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = .9) 
```

::: columns
::: {.column width="50%"}
Kept

```{r}
kept = rec %>% 
  prep() %>% 
  juice() %>% colnames()
kept
```
:::

::: {.column width="50%"}
Removed

```{r}
removed = !colnames(sim_lin_df) %in% kept
colnames(sim_lin_df)[removed]
```
:::
:::
:::

::: {.fragment .absolute left="0"}
```{r}
#| echo: true 
rec <- recipe(y ~ ., sim_lin_df) %>% 
  step_nzv(all_predictors()) %>% 
  step_lincomb(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = .7) 
```

::: columns
::: {.column width="50%"}
Kept

```{r}
kept = rec %>% 
  prep() %>% 
  juice() %>% colnames()
kept
```
:::

::: {.column width="50%"}
Removed

```{r}
removed = !colnames(sim_lin_df) %in% kept
colnames(sim_lin_df)[removed]
```
:::
:::
:::
:::

## Feature selection:

### Feature importance

::: r-stack
::: {.fragment .absolute .fade-in-then-out left="0"}
```{r}
#| echo: true
lr_fit <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(linear_reg()) %>% 
  fit(sim_lin_df)

tidy(lr_fit) %>% arrange(desc(p.value))
```
:::

::: {.fragment .absolute .fade-in-then-out left="0"}
```{r}
#| echo: true
rec2 <- rec %>% 
  step_rm(all_of("noise2"))

lr_fit <- workflow() %>% 
  add_recipe(rec2) %>% 
  add_model(linear_reg()) %>% 
  fit(sim_lin_df)

tidy(lr_fit) %>% arrange(desc(p.value))
```
:::

::: {.fragment .absolute left="0"}
```{r}
#| echo: true
rec2 <- rec %>% 
  step_rm(all_of(c("noise2","noise1")))

lr_fit <- workflow() %>% 
  add_recipe(rec2) %>% 
  add_model(linear_reg()) %>% 
  fit(sim_lin_df)

tidy(lr_fit) %>% arrange(desc(p.value))
```
:::
:::

## Feature selection:

### Feature importance

::: columns
::: {.column width="50%"}
::: {style="font-size:85%;"}
\

```{r}
#| echo: true
rf_spec <- 
  rand_forest(
    mode="regression",
    trees=500) %>% 
  set_mode("regression") %>% 
  set_engine(
    "ranger", 
    importance = "impurity")

rf_fit <- 
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(rf_spec) %>%  
  fit(sim_lin_df) 
```
:::
:::

::: {.column width="50%"}
```{r, fig.width=5, fig.height=5}
library(vip)
vip(rf_fit)
```
:::
:::

## Feature selection:

### Wrapper methods

A.k.a.:

-   Forwards addition
-   Backwards elimination

![](../imgs/wrappermethods.png)

## Feature Extraction

### Dimensionality Reduction

::: {style="font-size:70%;"}
::: columns
::: {.column width="50%"}
-   Principal Components Analysis (PCA)
-   Independent Components Analysis (ICA)
:::

::: {.column width="50%"}
-   Linear Discriminant Analysis (LDA)
-   t-distributed Stochastic Neighbor Embedding (t-SNE)
:::
:::
:::

![](../imgs/dimensionality_reduction.jpeg)

## PCA

```{r}
set.seed(123)
```

```{r}
#| echo: true
pca_rec <- recipe(y ~ ., sim_lin_df) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = 3)

pca_estimates <- prep(pca_rec, sim_lin_df)
pca_data <- bake(pca_estimates, sim_lin_df)

tidy(pca_estimates, number=3)
```

## PCA

```{r}
#| echo: true
tidy(pca_estimates, number=3, type = "variance") %>% filter(terms=="cumulative percent variance")
```

## PCA

```{r}
library(learntidymodels)
plot_loadings(pca_estimates, component_number <= 3)
```

## PCA

```{r}
tidy(pca_estimates, number=3) %>% 
  filter(component %in% c("PC1","PC2")) %>% 
  pivot_wider(names_from="component",values_from="value") %>% 
  ggplot(aes(x =PC1, y = PC2, color=factor(terms))) + 
    geom_jitter(size=5,width=.1) + 
    geom_hline(yintercept=0,linetype="dashed") + 
    geom_vline(xintercept=0,linetype="dashed")
```

## Applying these techniques to the Human Activity Recognition Data

```{r, echo=TRUE}
dim(df)
```

::: {style="font-size:50%;"}
```{r}
library(DT)
datatable(df, 
          options = list(
            pageLength = 20, # Number of rows to display
            scrollX = TRUE, # Enable horizontal scrollbar
            scrollY = '400px' # Set vertical scroll and its height
          ),
          width = '100%', # Table width
          height = 'auto' # Table height
)
```
:::

## Applying these techniques to the Human Activity Recognition Data

::: {style="font-size:75%"}
```{r}
split<-initial_split(df)
test<-testing(split)
train<-training(split)
```

```{r}
#| echo: true
mr_spec <- multinom_reg(
  mode="classification",
  engine="glmnet",
  penalty=0
)

fit_baseline <- workflow() %>% 
  add_formula(Activity ~ .) %>% 
  add_model(mr_spec) %>% 
  fit(train) 
```

Training Performance:

```{r}
fit_baseline %>% 
  augment(train) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```

Test Performance:

```{r}
fit_baseline %>% 
  augment(test) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```
:::

## Applying these techniques to the Human Activity Recognition Data

### Filtering

::: {style="font-size:75%"}
```{r}
#| echo: true
dim(train)
```

```{r}
#| echo: true
rec_filter <- rec <- recipe(Activity ~ ., df) %>% 
  step_nzv(all_predictors()) %>% 
  step_lincomb(all_predictors()) %>% 
  step_corr(all_predictors(), threshold = .9) 

prep(rec) %>% juice() %>% dim()
```
:::


## Applying these techniques to the Human Activity Recognition Data
### Filtering

::: {style="font-size:75%"}

```{r}
#| echo: true
fit_filter <- workflow() %>% 
  add_recipe(rec_filter) %>% 
  add_model(mr_spec) %>% 
  fit(train) 
```

Training Performance:

```{r}
fit_filter %>% 
  augment(train) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```

Test Performance:

```{r}
fit_filter %>% 
  augment(test) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```

:::

## Applying these techniques to the Human Activity Recognition Data
### Keeping most important features
::: {style="font-size:75%"}

::: columns
::: {.column width="50%"}
::: {style="font-size:85%;"}
\

```{r}
#| echo: true
rf_spec <- 
  rand_forest(
    trees=500) %>% 
  set_mode("classification") %>% 
  set_engine(
    "ranger", 
    importance = "impurity")

rec <- recipe(Activity ~ ., train) %>% 
  step_normalize(all_predictors())

rf_fit <- 
  workflow() %>% 
  add_recipe(rec) %>%
  add_model(rf_spec) %>%  
  fit(train) 

# keep = vi(rf_fit)$Variable[1:10]

keep = vi(rf_fit) %>% filter(Importance >= 50) %>% select(Variable)
keep = keep[[1]]

rec_importance <- recipe(Activity ~ ., train) %>% 
  step_normalize(all_predictors()) %>% 
  step_select(all_of(c(keep,"Activity"))) 
```
:::
:::

::: {.column width="50%"}
```{r, fig.width=5, fig.height=5}
library(vip)
vip(rf_fit)
```
:::
:::

:::

## Applying these techniques to the Human Activity Recognition Data
### Keeping most important features

::: {style="font-size:75%"}

```{r}
#| echo: true
fit_importance <- workflow() %>% 
  add_formula(Activity ~.) %>% 
  add_model(mr_spec) %>% 
  fit(prep(rec_importance) %>% bake(train)) 
```

Training Performance:

```{r}
fit_importance %>% 
  augment(prep(rec) %>% bake(train)) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```

Test Performance:

```{r}
fit_importance %>% 
  augment(prep(rec) %>% bake(test)) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```
:::

## Applying these techniques to the Human Activity Recognition Data
### PCA

::: {style="font-size:75%"}
```{r}
set.seed(123)
```

```{r}
#| echo: true
rec_pca <- recipe(Activity ~ ., df) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_pca(all_predictors(), threshold=.95)

pca_estimates <- prep(rec_pca, df)
pca_data <- bake(pca_estimates, df)

tidy(pca_estimates, number=3, type = "variance") %>% filter(terms=="cumulative percent variance")
```
:::

## Applying these techniques to the Human Activity Recognition Data
### PCA

::: {style="font-size:75%"}
```{r}
pca_split <- initial_split(pca_data, strata=Activity)
pca_train <- training(pca_split)
pca_test <- testing(pca_split)

fit_pca <- workflow() %>% 
  add_formula(Activity ~.) %>% 
  add_model(mr_spec) %>% 
  fit(pca_train) 
```

Training Performance:

```{r}
fit_pca %>% 
  augment(pca_train) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```

Test Performance:

```{r}
fit_pca %>% 
  augment(pca_test) %>% 
  metrics(truth=Activity,estimate=.pred_class)
```
:::
---
title: | 
  | Techniques for Avoiding Overfitting, Pt. 1
  | Regularization & Penalized Logistic Regression
subtitle: | 
  | Behavioral Data Science in R II
  | Unit 2
  | Module 6
format: 
  revealjs:
    css: style.css
---

## Regularization

```{r}
library(tidymodels)
library(tidyverse)
```


Techniques to avoid overfitting by increasing model bias in exchange for decreased variance.

![](../imgs/BV_tradeoff.png)

## Penalized Logistic regression

Mean Squared Error Loss Function:
$$
\frac{1}{n}\sum_{i}^{N}(y_i - f(x_i))^2
$$

L1 Regularization (Lasso):
$$
\frac{1}{n}\sum_{i}^{N}(y_i - f(x_i))^2 + \sum_{j} |\beta_j|
$$

L2 Regularization (Ridge):
$$
\frac{1}{n}\sum_{i}^{N}(y_i - f(x_i))^2 + \sum_{j} \beta_j^2
$$
## Penalized Logistic regression
:::{.columns}
::::{.column width="50%"}
![](../imgs/l1_regularization.webp)
::::

::::{.column width="50%"}
![](../imgs/l2_regularization.webp)

::::
:::

##

```{r}
set.seed(3)

b1 = seq(-1,4, by = .1)
b2 = seq(-1,4, by = .1)

penalty = 3.5

l1 <- tibble(crossing(b1,b2)) %>% mutate(z = penalty*(abs(b1) + abs(b2)))


z = b1^2 + b2^2
l2 <- tibble(crossing(b1,b2)) %>% mutate(z = penalty*(b1^2 + b2^2))

x1 = rnorm(100,mean=0,sd=1)
x2 = rnorm(100,mean=0,sd=1)
y = 5 + 2*x1 + 3*x2

mse <-tibble(crossing(b1,b2)) 
mse$err = 0

err <- function(b1,b2){
  p_y = b1*x1 + b2*x2
  mse = mean((y-p_y)^2)
}

for(i in 1:nrow(mse)){
  b1 = mse$b1[i]
  b2 = mse$b2[i]
  cur_err = err(b1,b2)
  mse$err[i]<-cur_err
}

l1 <- l1 %>% left_join(mse, by = join_by(b1,b2)) %>% mutate(sum = err + z) %>% mutate(min=ifelse(round(sum,digits=2)==min(round(sum,digits=2)),1,0))
l1min <- l1 %>% filter(min == 1)

l2 <- l2 %>% left_join(mse, by = join_by(b1,b2)) %>% mutate(sum = err + z) %>% mutate(min=ifelse(sum==min(sum),1,0))
l2min <- l2 %>% filter(min == 1)

mse <- mse %>% mutate(min = ifelse(err == min(err),1,0))
msemin <- mse %>% filter(min==1)
```

:::{.columns}
::::{.column width="33%"}
```{r, fig.width=4, fig.height=4}
ggplot(mse, aes(x = b1, y =b2)) 
```

```{r, fig.width=4, fig.height=4}
ggplot(mse, aes(x = b1, y =b2, fill=err)) + 
  geom_tile() + 
  scale_fill_continuous(type="viridis", guide="none") + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(data=msemin, mapping=aes(x=b1,y=b2),color="red",size=5)
```
::::

::::{.column width="33%"}
```{r, fig.width=4, fig.height=4}
ggplot(l1, aes(x = b1, y =b2, fill=z)) + 
  geom_tile() + 
  scale_fill_continuous(type="viridis", guide="none") + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0)
```
```{r, fig.width=4, fig.height=4}
ggplot(l1, aes(x = b1, y =b2, fill=sum)) + 
  geom_tile() + 
  scale_fill_continuous(type="viridis", guide="none") + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(data=l1min, mapping=aes(x=b1,y=b2),color="red",size=5)
```

::::

::::{.column width="33%"}
```{r, fig.width=4, fig.height=4}
ggplot(l2, aes(x = b1, y =b2, fill=z)) +
  geom_tile() + 
  scale_fill_continuous(type="viridis", guide="none") + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0)
```

```{r, fig.width=4, fig.height=4}
ggplot(l2, aes(x = b1, y =b2, fill=sum)) +
  geom_tile() + 
  scale_fill_continuous(type="viridis", guide="none") + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  geom_point(data=l2min, mapping=aes(x=b1,y=b2),color="red",size=5)
```
::::
:::


## Human Activity Recognition Data

```{r}
load('~/Documents/Github/BDSR2/data/HumanActivityRecognition.Rdata') 
df <- df %>% relocate(Activity)
```

```{r, echo=TRUE}
dim(df)
```

::: {style="font-size:50%;"}
```{r}
library(DT)
datatable(df, 
          options = list(
            pageLength = 20, # Number of rows to display
            scrollX = TRUE, # Enable horizontal scrollbar
            scrollY = '400px' # Set vertical scroll and its height
          ),
          width = '100%', # Table width
          height = 'auto' # Table height
)
```
:::

```{r}
library(themis)
recipe(Activity ~ ., df) %>% 
  step_downsample(Activity, under_ratio = 1) %>% 
  prep() %>%
  bake(new_data = NULL) %>%
  ggplot(aes(x = Activity, fill = Activity))+geom_bar() +
  geom_bar()
```

```{r}
split <- initial_split(df)
train <- training(split)
test <- testing(split)

rec <- recipe(Activity ~ ., df) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_downsample(Activity,under_ratio=1)

rec %>% prep() %>% bake(new_data = NULL) %>% dim()

split <- initial_split(rec %>% prep() %>% bake(new_data = NULL))
train <- training(split)
test <- testing(split)

rec <- recipe(Activity ~ ., train) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors()) %>% 
  step_downsample(Activity,under_ratio=1)

mnr_spec <- multinom_reg(
  mode="classification",
  engine="glmnet",
  penalty=0
)

mnr_wf <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mnr_spec) 

mnr_fit <- mnr_wf %>% 
  fit(train) 

mnr_fit %>% augment(train) %>% accuracy(truth=Activity, estimate=.pred_class)

mnr_fit %>% augment(test) %>% accuracy(truth=Activity, estimate=.pred_class)
```

```{r, fig.width=3, fig.height=4}
mnr_spec_tune <- multinom_reg(
  mode="classification",
  engine="glmnet",
  penalty = tune(),
  mixture = 1
)

folds = vfold_cv(train,v=5)

param_grid = grid_regular(penalty(), levels=50) #tibble(penalty = 10^seq(-3, 5, length.out = 50))

tune_wf <- workflow() %>%
  add_recipe(rec) %>% 
  add_model(mnr_spec_tune)

tune_rs <- tune_grid(
  tune_wf,
  folds,
  grid = param_grid,
  metrics = metric_set(accuracy)
)

collect_metrics(tune_rs)

autoplot(tune_rs)
```

```{r}
chosen_penalty <- tune_rs %>% select_best(metric = "accuracy", penalty)

final_mnr <- finalize_workflow(tune_wf, chosen_penalty)

final_fit <- fit(final_mnr, train)

final_fit %>% augment(train) %>% accuracy(truth=Activity, estimate=.pred_class)

final_fit %>% augment(test) %>% accuracy(truth=Activity, estimate=.pred_class)
```

## Regression Example
### <span style="font-size:60%;">Shows how <U>model complexity</U> relates to over-/under-fitting</span>

```{r}
set.seed(1)
```

```{r}
#| echo: true
n_obs = 100
sim_lin_df = 
  tibble(
    x1 = rnorm(n_obs,0,5),
    x2 = rnorm(n_obs,0,10),
    x3 = rnorm(n_obs,0,15),
    y = 4.17 + 8.06*x1 + 8.06*x2 + 8.06*x3 + rnorm(n_obs,0,10)
    ) %>% 
  mutate(
    noise1 = rnorm(n_obs,0,10), 
    noise2 = rnorm(n_obs,0,10), 
    perfectCorr_x2 = 2.55 + 3.94*x2, 
    highCorr_x3 = 7.92 + 4.37*x3 + rnorm(n_obs,0,40), 
    linComb_x2_x3 = 17 + 3.1*x2 + 4.7*x3, # x6 is a linear combination of x1 and x2
    zeroVar = rep(1.24, n_obs), # x5 has zero variance
  ) 
```

```{r}
split<-initial_split(sim_lin_df)
test<-testing(split)
train<-training(split)

lr_spec <- linear_reg()

rec <- recipe(y ~ ., train) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors())

lr_wf <- workflow() %>% 
  add_model(lr_spec) %>% 
  add_recipe(rec)

lr_fit <- lr_wf %>% 
  fit(train)

tidy(lr_fit)
```

```{r}
lr_spec_tune <- linear_reg(
  mode="regression",
  engine="glmnet",
  penalty = tune(),
  mixture = 1
)

folds = vfold_cv(train,v=5)

param_grid = tibble(penalty = 10^seq(-3, 5, length.out = 100))

rec <- recipe(y ~ ., train) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_predictors())

tune_wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(lr_spec_tune)

tune_rs <- tune_grid(
  tune_wf,
  folds,
  grid = param_grid
)

collect_metrics(tune_rs) %>% filter(.metric=="rmse", mean < 10.4) %>% arrange(desc(penalty))

autoplot(tune_rs)

```

```{r}

chosen_penalty <- tune_rs %>% select_best(metric = "rsq", penalty)
chosen_penalty$penalty = 9.0

final_lr <- finalize_workflow(tune_wf, chosen_penalty)

final_fit <- fit(final_lr, train)

# lr_spec <- linear_reg(
#   mode="regression",
#   engine="glmnet",
#   penalty=1.4,
#   mixture=1
# )
# 
# final_fit <- workflow() %>% 
#   add_recipe(rec) %>% 
#   add_model(lr_spec) %>% 
#   fit(train)

final_fit %>% 
  augment(train) %>% 
  metrics(truth=y, estimate = .pred)

final_fit %>% 
  augment(test) %>% 
  metrics(truth=y, estimate = .pred)

tidy(final_fit)
```

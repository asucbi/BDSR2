---
title: "Interpretability and Feature Importance"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
bibliography: ../references.bib
---

## Interpretability

- As scientists, we don't want to only know **what** prediction to make but also **why** a prediction would be made
- We want to use machine learning models to gain insight and understanding into a domain
- We want our models to be **interpretable**

## Interpretability in applications

::: {style="font-size: 80%;"}

:::: {.columns}
::: {.column width="50%"}
- Sometimes non-scientists just want predictions:
  - A bank wants to process credit card applications
  - A social media co. wants to predict which users will churn
  - A doctor wants to predict which drug will have the best effect for a patient
:::
::: {.column width="50%"}

::: {.fragment}
- Still, applications often need interpretability:
  - Customers and want to know why a loan was denied
  - Companies want to be able to act to improve retention
  - We want to ensure **fairness** across e.g. race or gender
:::

:::
::::
:::


## Black boxes

Models like Random Forests and Boosted trees are sometimes likened to black boxes:

![](../imgs/mermaid-diagram-bb.svg)
<!-- This isn't working for some reason but works fine on mermaid website??? -->


## Interpretability tools

We need ways to look inside the black box

- Feature importance
- Diagnostic plots

## Feature importance

How important is feature $x_i$ for the predictions of the model?

::: {.fragment}
**Permutation Feature Importance**

- How much does scrambling (permuting) the values of feature $x_i$ hurt the model's predictions?

:::


## Permutation Feature Importance

::: {.callout-tip icon="false"}
## Algorithm

For any given loss function do the following:

1. Compute loss function for original model
2. For variable $i$ in $\{1,...,p\}$ do:
     - randomize values of the predictor
     - apply given ML model
     - estimate loss function
     - compute feature importance as some difference/ratio measure 
       between permuted loss vs. original loss
3. Sort variables by descending feature importance 
:::

# Feature Importance for NYC taxi model

```{r}
#| message: false
#| include: false
library(tidyverse)
library(tidymodels)
library(hutils)
library(DALEX)
library(DALEXtra)
taxi <- read_csv("data/nyc-taxi.csv") %>% 
  mutate(pickup_datetime = lubridate::as_datetime(pickup_datetime))
```

```{r}
#| include: false
splits <- initial_split(taxi, prop = .8)
train <- training(splits)
test <- testing(splits)

folds <- vfold_cv(train, 10)
```

```{r}
#| include: false
train %>% 
  ggplot(aes(x=pickup_longitude, y = pickup_latitude)) +
  geom_point(alpha = .2, size = .2, shape = 16, color = "goldenrod1") +
  theme_void() +
  theme(panel.background = element_rect(fill = "black"))
```


```{r}
#| include: false
fe_rec <- recipe(trip_duration ~ ., train) %>%
  step_mutate(
    dist = haversine_distance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude),
    bearing = bearing(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude)
    ) %>%
  step_date(pickup_datetime, features = c("dow", "doy")) %>%
  step_time(pickup_datetime, features = c("decimal_day")) %>% 
  update_role(pickup_datetime, new_role = "id")

train_juiced <- juice(prep(fe_rec))
```

```{r}
#| include: true
#| cache: true
rf_mod <- rand_forest(mode = "regression")

wf <- workflow() %>% 
  add_recipe(fe_rec) %>% 
  add_model(rf_mod)

fitted_model <- wf %>% 
  fit(train)

```

## `DALEX`

- Prep fitted workflow into an `explainer` object
  - I have provided a helper function for this: `make_explainer_obj()`
- `DALEX` functions:
  - `model_part()`: permutation feature importance
  - `model_profile()`: PDP plots

## Permutation Feature Importance

```{r}
#| include: false
make_explainer_obj <- function(fitted_workflow){
  fitted_model <-
    fitted_workflow %>% 
    extract_fit_parsnip() # <- parsnip model_fit object

  feature_data <- 
    fitted_workflow %>% 
    extract_mold() %>% 
    pluck("predictors") 
  
  outcome_data <- 
    fitted_workflow %>% 
    extract_mold() %>% 
    pluck("outcomes") %>% 
    pluck(1)    # <- is is a 1D df, make it a vector
  
  vip_features <- 
    explain_tidymodels(
      fitted_model, 
      data = feature_data, 
      y = outcome_data
    )
  
  return(vip_features)
}

```

```{r}
#| message: false
#| warning: false
#| output: false
#| echo: true

library(DALEX)
library(DALEXtra)

explainer_rf <- make_explainer_obj(fitted_model)
vip_res <- model_parts(explainer_rf, N = 1000)
```
```{r}
#| echo: false

ggplot_imp <- function(...) {
  obj <- list(...)
  metric_name <- attr(obj[[1]], "loss_name")
  metric_lab <- paste(metric_name, 
                      "after permutations\n(higher indicates more important)")
  
  full_vip <- bind_rows(obj) %>%
    filter(variable != "_baseline_")
  
  perm_vals <- full_vip %>% 
    filter(variable == "_full_model_") %>% 
    group_by(label) %>% 
    summarise(dropout_loss = mean(dropout_loss))
  
  p <- full_vip %>%
    filter(variable != "_full_model_") %>% 
    mutate(variable = fct_reorder(variable, dropout_loss)) %>%
    group_by(variable) %>% 
    mutate(
      M = mean(dropout_loss),
      min_loss = perm_vals %>% pull(dropout_loss)
      ) %>% 
    ggplot(aes(dropout_loss, variable)) 
  if(length(obj) > 1) {
    p <- p + 
      facet_wrap(vars(label)) +
      geom_segment(aes(xend = min_loss, x = M, yend = variable ), linewidth = 8, color = "dodgerblue2") +
      geom_boxplot(aes(color = label, fill = label), alpha = 1, coef = 500, width = .33)
  } else {
    p <- p + 
      geom_segment(aes(xend = min_loss, x = M, yend = variable ), linewidth = 8, color = "dodgerblue2") +
      geom_boxplot(fill = "dodgerblue4", color = "dodgerblue4", alpha = 1, coef = 500, width = .33) +
      scale_x_continuous(limits = c(perm_vals %>% pull(dropout_loss), NA))
    
  }
  p +
    theme(legend.position = "none") +
    labs(x = metric_lab, 
         y = NULL,  fill = NULL,  color = NULL)
}
```


```{r}
#| echo: true
ggplot_imp(vip_res)
```


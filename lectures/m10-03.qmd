---
title: "Text Embeddings"
subtitle: Behavioral Data Science in R II
format: 
  revealjs:
    css: style.css
bibliography: ../references.bib
---

## Embeddings: Vector Representations

An **embedding** is a representation of something as a vector of numbers

## Term frequency embeddings

Term-frequency embeddings are a way to represent documents.

```{r}
#| echo: false
library(tidyverse)
library(patchwork)

tf_df <- tibble(
  doc = c("A", "B", "C", "D"),
  mozarella = c(0, 1, 5, 81),
  pepperoni = c(0, 1, 7, 63),
  tidy = c(5, 41, 1, 4),
  calculate = c(9, 55, 0, 1),
  bake = c(3, 12, 3, 45),
)

tf_df
```

## Document similarity

The similarity between documents can be best calculated using cosine similarity.

```{r}
#| message: false

tf_dists <- cross_join(tf_df, tf_df) %>% 
  pivot_longer(cols = c(-doc.x, -doc.y), names_to = "word", values_to = "count") %>% 
  mutate(
    count_from = str_sub(word, -1, -1),
    word = str_sub(word, 0, -3)
    ) %>% 
  pivot_wider(names_from = count_from, values_from = count) %>% 
  group_by(`doc.x`, `doc.y`) %>% 
  summarize(
    euclidean = sqrt(sum((x - y)^2)),
    cos_sim = sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2)))
    ) %>% 
  ungroup() %>% 
  pivot_longer(c(euclidean, cos_sim))

tf_dists %>% 
  filter(name == "cos_sim") %>% 
  ggplot(aes(x=doc.x, y = doc.y, fill = value)) +
  geom_tile() +
  scale_fill_viridis_c() +
  labs(fill = "similarity", title = "Cosine Similarity") + 
  scale_y_discrete(limits = rev) +
  theme_minimal() +
  theme(aspect.ratio = 1) +


tf_dists %>% 
  filter(name == "euclidean") %>% 
  ggplot(aes(x=doc.x, y = doc.y, fill = value)) +
  geom_tile()  +
  scale_fill_viridis_c() +
  labs(fill = "distance", title = "Euclidean Distance") +
  scale_y_discrete(limits = rev) +
  theme_minimal() +
  theme(aspect.ratio = 1)
```

## Word Embeddings

- Word embeddings represent the meaning of a word as a vector of numbers
- Different approaches to building, e.g.:
  - Word2Vec [@mikolov2013]
  - GloVE [@pennington.etal2014]
- But all based on the co-occurrence of words together in a corpus of real text

```{r}
#| echo: false
glove_df <- read_delim(
  "local/glove.6B.50d.txt", 
  delim = " ",
  col_names = c("word", paste0("dim", sprintf("%02d", seq(1,50,1)))),
  quote = ""
  )

glove <- function(text){
  glove_df %>% 
    filter(word==text) %>% 
    select(-word) %>% 
    as.numeric()
}

cos_sim <- function(x, y){
  sum(x*y)/(sqrt(sum(x^2))*sqrt(sum(y^2)))
}

```

## GloVe Embedding

Two computer representation of "queen":

:::: {.columns}
::: {.column width="50%"}

**GloVe**

```{r}
#| echo: true
#| eval: false

glove("queen")
```

```{r}
round(glove("queen"), 2)
```
:::
::: {.column width="50%"}

**DALL-E 3**

![](../imgs/queen - DALLÂ·E 2024-03-09.webp)
:::
::::


## Calculating cosine similarity

```{r}
#| echo: true

cos_sim(glove("fortress"), glove("castle"))

cos_sim(glove("grape"), glove("cherry"))

cos_sim(glove("grape"), glove("castle"))

cos_sim(glove("cherry"), glove("fortress"))
```


## Capturing meaning with word embeddings

![](../imgs/glove-example.png)


## Capturing meaning with word embeddings in `R`

```{r}
#| echo: true
cos_sim(
  glove("queen") - glove("king") ,
  glove("woman") - glove("man")
  )

cos_sim(
  glove("phoenix") - glove("arizona"),
  glove("milwaukee") - glove("wisconsin")
  )
```

# Creating word embeddings

## Compression as understanding

::: {style="font-size: 65%;"}
> Presumably there are some generalities to be found in there, something more concise and descriptive than 8.5 billion completely independent and unrelated ratings. [...] A fun property of machine learning is that this reasoning works in reverse too: If meaningful generalities can help you represent your data with fewer numbers, finding a way to represent your data in fewer numbers can often help you find meaningful generalities. Compression is akin to understanding and all that. 
> 
> ---Simon Funk (aka Brandyn Webb)

:::

::: callout-tip
## Further reading
See [@rissanen1978] further writings on the idea of **minimum descriptive length** for more formal treatments.
:::

## How to make word embeddings

- Start with a large corpus of naturalistic text
- Count up the frequencies of words and word co-occurrences
- Tabulate these into a big table (matrix) of word-word co-occurrence probabilities 
- Translate probabilities into Pointwise Mutual Information (PMI)
- "Factorize" this matrix (table) into two new matrices
- Keep $k$ columns of the first of these matrices: the rows are your word embedding vectors

## Skip-grams

- Measure probability of occurrence of word $w_2$ in context of word $w_1$: $P(w_2|w_1)$
- For skip-grams, "context" is within some number of words (allowing for "skipping" of words)
  - E.g. within 5 words, $\text{dog}|\text{fox}$ in  "the quick brown **fox** jumped over the lazy **dog**."
  
## Pointwise Mutual Information

::: {.callout-caution icon="false"}
## Definition: Pointwise Mutual Information

$$\text{pmi}(x;y) = \text{log}_2 \frac{p(x,y)}{p(x)p(y)} =  \text{log}_2 \frac{p(x|y)}{p(x)} $$
:::

- Measure of association between discrete variables
- Compares the probability of two events occurring together to what this probability would be if the events were independent
- PMI is symmetric: $\text{pmi}(x;y) = \text{pmi}(y;x)$

## Matrix factorization
:::: {.columns}
::: {.column width="50%"}
- Get a big term-term matrix of skip-gram PMI
- "Factorize" this matrix
  - i.e.: split it apart
:::
::: {.column width="50%"}
|    | w1 | w2 | w3 | w4 |
|----|----|----|----|----|
| w1 | .5 | .1 | .9 | .1 |
| w2 | .1 | .3 | .8 | .9 |
| w3 | .4 | .8 | .4 | .2 |
| w4 | .4 | .6 | .7 | .7 |
:::
::::


## Matrix multiplication basics

![](../imgs/matrix-mult-demo.png)


## Matrix decomposition
::: {style="font-size: 80%;"}

- We can reverse the process of matrix multiplication to pull a matrix apart into the product of several matrices
- One method is called the Singular Value Decomposition
  - (closely related to PCA)

:::

![](../imgs/svd-example-tai-dane-bradley.jpg){fig-align:"center"}

::: {style="font-size: 50%;"}
(credit: Tai-Dane Bradley)
:::


## Making word vectors

Choose an embedding dimension, keep only first $k$ rows/columns of the U matrix (ordered by "importance" aka singular values)

![](../imgs/building-word-vecs.png)

## Visualizing word vectors

```{r}
glove_df[100:200,0:25] %>% 
  pivot_longer(-word) %>% 
  ggplot(aes(y = word, x= name, fill = value)) +
  geom_tile() +
  scale_fill_viridis_c() +
  theme_minimal(base_size = 7) +
  theme(aspect.ratio = 2) +
  labs(x = "Dimension", y = "Token")
  
```

---

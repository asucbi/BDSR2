@book{gareth.etal2014,
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
title = {An Introduction to Statistical Learning: With Applications in R},
year = {2014},
isbn = {1461471370},
publisher = {Springer Publishing Company, Incorporated},
abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.}
}

@article{maclehose.etal2011,
  title = {Turning the {{Bayesian Crank}}},
  author = {MacLehose, Richard F. and Oakes, J. Michael and Carlin, Bradley P.},
  year = {2011},
  month = may,
  journal = {Epidemiology (Cambridge, Mass.)},
  volume = {22},
  number = {3},
  pages = {365--367},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e318212b31a},
  urldate = {2024-01-05},
  pmcid = {PMC5659857},
  pmid = {21464652},
  file = {/Users/derekpowell/Zotero/storage/DNFRTGGQ/MacLehose et al-2011-Turning the Bayesian Crank.pdf}
}

@incollection{strevens2006,
  title = {The {{Bayesian Approach}} to the {{Philosophy}} of {{Science}}},
  booktitle = {Encyclopedia of {{Philosophy}}, Second Edition},
  author = {Strevens, Michael},
  editor = {Borchert, D. M.},
  year = {2006},
  pages = {495--502},
  publisher = {{Macmillan Reference}},
  file = {/Users/derekpowell/Zotero/storage/4WV5FZDP/Strevens-2006-The Bayesian Approach to the Philosophy of Science.pdf;/Users/derekpowell/Zotero/storage/H4XUJG34/STRTBA.html}
}

@article{etz.vandekerckhove2018,
  title = {Introduction to {{Bayesian Inference}} for {{Psychology}}},
  author = {Etz, Alexander and Vandekerckhove, Joachim},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {5--34},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1262-3},
  urldate = {2024-01-05},
  langid = {english},
  file = {/Users/derekpowell/Zotero/storage/44W6XRY8/Etz_Vandekerckhove-2018-Introduction to Bayesian Inference for Psychology.pdf}
}

@article{gelman.etal2004,
  title = {Direct {{Data Manipulation}} for {{Local Decision Analysis}} as {{Applied}} to the {{Problem}} of {{Arsenic}} in {{Drinking Water}} from {{Tube Wells}} in {{Bangladesh}}},
  author = {Gelman, Andrew and Trevisani, Matilde and Lu, Hao and Van Geen, Alexander},
  year = {2004},
  month = dec,
  journal = {Risk Analysis},
  volume = {24},
  number = {6},
  pages = {1597--1612},
  issn = {0272-4332, 1539-6924},
  doi = {10.1111/j.0272-4332.2004.00553.x},
  urldate = {2024-01-17},
  abstract = {A wide variety of tools are available, both parametric and nonparametric, for analyzing spatial data. However, it is not always clear how to translate statistical inferences into decision recommendations. This article explores the possibilities of estimating the effects of decision options using very direct manipulation of data, bypassing formal statistical analysis. We illustrate with the application that motivated this research, a study of arsenic in drinking water in nearly 5,000 wells in a small area in rural Bangladesh. We estimate the potential benefits of two possible remedial actions: (1) recommendations that people switch to nearby wells with lower arsenic levels; and (2) drilling new community wells. We use simple nonparametric clustering methods and estimate uncertainties using cross-validation.},
  langid = {english},
  file = {/Users/dmpowell/Zotero/storage/8BFESJBN/Gelman et al. - 2004 - Direct Data Manipulation for Local Decision Analys.pdf}
}

@article{rissanen1978,
title = {Modeling by shortest data description},
journal = {Automatica},
volume = {14},
number = {5},
pages = {465-471},
year = {1978},
issn = {0005-1098},
doi = {https://doi.org/10.1016/0005-1098(78)90005-5},
url = {https://www.sciencedirect.com/science/article/pii/0005109878900055},
author = {J. Rissanen},
keywords = {Modeling, parameter estimation, identification, statistics, stochastic systems},
abstract = {The number of digits it takes to write down an observed sequence x1, â€¦, xN of a time series depends on the model with its parameters that one assumes to have generated the observed data. Accordingly, by finding the model which minimizes the description length one obtains estimates of both the integer-valued structure parameters and the real-valued system parameters.}
}

@misc{mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3781},
  urldate = {2023-12-04},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/dmpowell/Zotero/storage/PCPXZHPD/Mikolov et al-2013-Efficient Estimation of Word Representations in Vector Space.pdf;/Users/dmpowell/Zotero/storage/UR7DC8ZJ/1301.html}
}

@inproceedings{pennington.etal2014,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}
